{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# install talib","metadata":{}},{"cell_type":"code","source":"# !cp ../input/talibinstall/ta-lib-0.4.0-src.tar.gzh  ./ta-lib-0.4.0-src.tar.gz\n# !tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n# !cd ta-lib && ./configure --prefix=/usr > null && make  > null && make install > null\n# !cp ../input/talibinstall/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n# !pip install TA-Lib-0.4.21.tar.gz > null\n# !pip install ../input/talibinstall/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl >null\n# import talib\n# talib.__version__","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:25.564981Z","iopub.execute_input":"2022-01-18T04:54:25.565291Z","iopub.status.idle":"2022-01-18T04:54:25.56968Z","shell.execute_reply.started":"2022-01-18T04:54:25.565204Z","shell.execute_reply":"2022-01-18T04:54:25.569013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### メモリ削減","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:25.570906Z","iopub.execute_input":"2022-01-18T04:54:25.571313Z","iopub.status.idle":"2022-01-18T04:54:25.589728Z","shell.execute_reply.started":"2022-01-18T04:54:25.571277Z","shell.execute_reply":"2022-01-18T04:54:25.58897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import psutil","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:25.591021Z","iopub.execute_input":"2022-01-18T04:54:25.591275Z","iopub.status.idle":"2022-01-18T04:54:25.600288Z","shell.execute_reply.started":"2022-01-18T04:54:25.59124Z","shell.execute_reply":"2022-01-18T04:54:25.599571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 特徴量エンジニアリング","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport talib\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nimport time\n\nclass Feature():\n\n    def __init__(self) -> None:\n        pass\n    \n    def __del__(self):\n        \"\"\"オブジェクトが破棄されるとき呼び出される\"\"\"\n        print('Feature died:', id(self))\n    def MACD(close : pd.DataFrame, fastperiod=324, slowperiod=296, signalperiod=272):\n        exp1 = close.rolling(fastperiod).mean()\n        exp2 = close.rolling(slowperiod).mean()\n        macd = 100 * (exp1 - exp2) / exp2\n        signal = macd.rolling(signalperiod).mean()\n        hist = signal - macd\n\n        return hist\n\n    def conv_data(self, df, Asset_ID, df_list, save_fet=False, save_name='feature', save_mem=True, test=False):\n        \n        df.fillna(method='ffill', inplace=True)\n        \n        if not test:\n        \n            for i in range(13):\n                if not (i == Asset_ID and i == 0 and i == 7):\n                    as_df = df_list[i].copy()\n                    #df[str(i)+'VWAP_shift105'] = as_df['VWAP'].shift(105) / as_df[\"VWAP\"]\n                    #df[str(i)+'MOM'] = talib.MOM(as_df['Close'], timeperiod=176)\n\n                    del as_df\n                    gc.collect()\n\n            df['VWAP_shift15'] = df['VWAP'].shift(15) / df[\"VWAP\"]\n            #df['VWAP_shift30'] = df['VWAP'].shift(30) / df[\"VWAP\"]\n            df['VWAP_shift45'] = df['VWAP'].shift(45) / df[\"VWAP\"]\n            df['VWAP_shift90'] = df['VWAP'].shift(90) / df[\"VWAP\"]\n            #df['VWAP_shift120'] = df['VWAP'].shift(120) / df[\"VWAP\"]\n            df['VWAP_shift180'] = df['VWAP'].shift(180) / df[\"VWAP\"]\n            df['VWAP_shift210'] = df['VWAP'].shift(210) / df[\"VWAP\"]\n            #df['VWAP_shift240'] = df['VWAP'].shift(240) / df[\"VWAP\"]\n            df['VWAP_shift310'] = df['VWAP'].shift(310) / df[\"VWAP\"]\n            df['VWAP_shift350'] = df['VWAP'].shift(350) / df[\"VWAP\"]\n            #df['VWAP_shift400'] = df['VWAP'].shift(400) / df[\"VWAP\"]\n            df['VWAP_shift450'] = df['VWAP'].shift(450) / df[\"VWAP\"]\n#             df['VWAP_shift550'] = df['VWAP'].shift(550) / df[\"VWAP\"]\n#             df['VWAP_shift600'] = df['VWAP'].shift(600) / df[\"VWAP\"]\n#             df['VWAP_shift650'] = df['VWAP'].shift(650) / df[\"VWAP\"]\n#             #df['VWAP_shift750'] = df['VWAP'].shift(750) / df[\"VWAP\"]\n#             df['VWAP_shift800'] = df['VWAP'].shift(800) / df[\"VWAP\"]\n#             df['VWAP_shift1000'] = df['VWAP'].shift(1000) / df[\"VWAP\"]\n#             df['VWAP_shift1500'] = df['VWAP'].shift(1500) / df[\"VWAP\"]\n#             df['VWAP_shift2000'] = df['VWAP'].shift(2000) / df[\"VWAP\"]\n#             df['VWAP_shift2500'] = df['VWAP'].shift(2500) / df[\"VWAP\"]\n#             df['VWAP_shift3000'] = df['VWAP'].shift(3000) / df[\"VWAP\"]\n#             df['VWAP_shift3500'] = df['VWAP'].shift(3500) / df[\"VWAP\"]\n#             df['VWAP_shift4000'] = df['VWAP'].shift(4000) / df[\"VWAP\"]\n#             df['VWAP_shift4500'] = df['VWAP'].shift(4500) / df[\"VWAP\"]\n#             df['VWAP_shift5000'] = df['VWAP'].shift(5000) / df[\"VWAP\"]\n#             df['VWAP_shift5500'] = df['VWAP'].shift(5500) / df[\"VWAP\"]\n#             df['VWAP_shift6000'] = df['VWAP'].shift(6000) / df[\"VWAP\"]\n#             df['VWAP_shift7000'] = df['VWAP'].shift(7000) / df[\"VWAP\"]\n#             df['VWAP_shift8000'] = df['VWAP'].shift(8000) / df[\"VWAP\"]\n#             df['VWAP_shift9000'] = df['VWAP'].shift(9000) / df[\"VWAP\"]\n#             df['VWAP_shift10000'] = df['VWAP'].shift(10000) / df[\"VWAP\"]\n#             df['VWAP_shift11000'] = df['VWAP'].shift(11000) / df[\"VWAP\"]\n#             df['VWAP_shift12000'] = df['VWAP'].shift(12000) / df[\"VWAP\"]\n#             df['VWAP_shift14000'] = df['VWAP'].shift(14000) / df[\"VWAP\"]\n#             df['VWAP_shift16000'] = df['VWAP'].shift(16000) / df[\"VWAP\"]\n#             df['VWAP_shift18000'] = df['VWAP'].shift(18000) / df[\"VWAP\"]\n#             df['VWAP_shift25000'] = df['VWAP'].shift(25000) / df[\"VWAP\"]\n#             df['Volume_shift_sma2000'] = (df['Volume'].rolling(2000).sum() / df[\"Volume\"]).apply(np.log)\n#             df['Close_shift70'] = df['Close'].shift(70) / df[\"Close\"]\n#             df['Close_shift280'] = df['Close'].shift(280) / df[\"Close\"]\n#             df['SMA_18000_std'] = df['Close'].rolling(18000).std().shift() / df['Close']\n#             df['ROCP'] = talib.ROCP(df['Close'], timeperiod=181)\n            df['MOM'] = talib.MOM(df['Close'], timeperiod=176)\n            df['MOM_1700'] = talib.MOM(df['Close'], timeperiod=1760)\n            df['RSI'] = talib.RSI(df['Close'], timeperiod=206)\n            #df['EMA'] = (talib.EMA(df['Close'], timeperiod=11) - df['Close']) / df[\"Close\"]\n            df['APO'] = talib.APO(df['Close'], fastperiod=117, slowperiod=166, matype=0)\n            df['CMO'] = talib.CMO(df['Close'], timeperiod=204)\n            macdhist = self.MACD(df['Close'], fastperiod=324, slowperiod=296, signalperiod=272)\n            df['macdhist0'] = macdhist\n            df['macdhist1'] = macdhist.shift(1)\n            df['macdhist5'] = macdhist.shift(5)\n            df['macdhist15'] = macdhist.shift(15)\n            df['ADX'] = talib.ADX(df[\"High\"], df[\"Low\"], df[\"Close\"], timeperiod=181)\n            df['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(df[\"Close\"], timeperiod=181)\n            df['HT_DCPERIOD'] = talib.HT_DCPERIOD(df[\"Close\"])\n            df['NATR'] = talib.NATR(df[\"High\"], df[\"Low\"], df[\"Close\"], timeperiod=181)\n            df['AROONOSC'] = talib.AROONOSC(df[\"High\"], df[\"Low\"], timeperiod=181)\n            aroondown, aroonup = talib.AROON(df[\"High\"], df[\"Low\"], timeperiod=181)\n            df['aroondown'] = aroondown\n            df['aroonup'] = aroonup\n            df['PLUS_DI'] = talib.PLUS_DI(df[\"High\"], df[\"Low\"], df[\"Close\"], timeperiod=181)\n            upper1, middle,lower1 = talib.BBANDS(df[\"Close\"], timeperiod=181, nbdevup=3, nbdevdn=3, matype=0)\n            df['BBANDS'] = (upper1 - df['Close']).apply(np.log)\n            df['CDL2CROWS'] = talib.CDL2CROWS(df[\"Open\"], df[\"High\"], df[\"Low\"], df[\"Close\"])\n\n            categorical_features = ['CDL2CROWS']\n            \n            delete_columns = ['Asset_ID','High','Low','Close','Open','VWAP','Volume','Count']\n            #df.drop(delete_columns, axis=1, inplace=True)\n            for colum in delete_columns:\n                del df[colum]; gc.collect();\n                \n            df.reset_index(drop=True, inplace=True)\n        else:\n            \n            cols = []\n            for i in range(13):\n                if not (i == Asset_ID and i == 0 and i == 7):\n                    #cols.append(str(i)+'VWAP_shift105')\n                    #cols.append(str(i)+'MOM')\n                    pass\n            cols.extend(['VWAP_shift15','VWAP_shift45','VWAP_shift90','VWAP_shift180','VWAP_shift210','VWAP_shift310','VWAP_shift350','VWAP_shift450','Volume_shift_sma2000','Close_shift70','Close_shift280','SMA_18000_std','ROCP','MOM','MOM_1700','RSI','APO','CMO','macdhist0','macdhist1','macdhist5','macdhist15','ADX','AD','LINEARREG_ANGLE','HT_DCPERIOD','NATR','AROONOSC','aroondown','aroonup','PLUS_DI','BBANDS','CDL2CROWS'])\n            ans = pd.DataFrame(index=[0], columns=cols)\n            \n            start = time.time()\n            for i in range(13):\n                if not (i == Asset_ID and i == 0 and i == 7):\n                    as_df = df_list[i].copy()\n                    #ans[str(i)+'VWAP_shift105'] = as_df.iloc[-106,7] / as_df.iloc[-1,7]\n                    #ans[str(i)+'MOM'] = talib.MOM(as_df.iloc[-177:,5], timeperiod=176).iloc[-1]\n            print (\"1.0elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n\n            start = time.time()\n            ans['VWAP_shift15'] = df.iloc[-16,7] / df.iloc[-1,7]\n            #ans['VWAP_shift30'] = df.iloc[-31,7] / df.iloc[-1,7]\n            ans['VWAP_shift45'] = df.iloc[-46,7] / df.iloc[-1,7]\n            ans['VWAP_shift90'] = df.iloc[-91,7] / df.iloc[-1,7]\n            #ans['VWAP_shift120'] = df.iloc[-121,7] / df.iloc[-1,7]\n            ans['VWAP_shift180'] = df.iloc[-181,7] / df.iloc[-1,7]\n            ans['VWAP_shift210'] = df.iloc[-211,7] / df.iloc[-1,7]\n            #ans['VWAP_shift240'] = df.iloc[-241,7] / df.iloc[-1,7]\n            ans['VWAP_shift310'] = df.iloc[-311,7] / df.iloc[-1,7]\n            ans['VWAP_shift350'] = df.iloc[-351,7] / df.iloc[-1,7]\n            #ans['VWAP_shift400'] = df.iloc[-401,7] / df.iloc[-1,7]\n            ans['VWAP_shift450'] = df.iloc[-451,7] / df.iloc[-1,7]\n#             ans['VWAP_shift550'] = df.iloc[-551,7] / df.iloc[-1,7]\n#             ans['VWAP_shift600'] = df.iloc[-601,7] / df.iloc[-1,7]\n#             ans['VWAP_shift650'] = df.iloc[-651,7] / df.iloc[-1,7]\n#             #ans['VWAP_shift750'] = df.iloc[-751,7] / df.iloc[-1,7]\n#             ans['VWAP_shift800'] = df.iloc[-801,7] / df.iloc[-1,7]\n#             ans['VWAP_shift1000'] = df.iloc[-1001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift1500'] = df.iloc[-1501,7] / df.iloc[-1,7]\n#             ans['VWAP_shift2000'] = df.iloc[-2001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift2500'] = df.iloc[-2501,7] / df.iloc[-1,7]\n#             ans['VWAP_shift3000'] = df.iloc[-3001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift3500'] = df.iloc[-3501,7] / df.iloc[-1,7]\n#             ans['VWAP_shift4000'] = df.iloc[-4001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift4500'] = df.iloc[-4501,7] / df.iloc[-1,7]\n#             ans['VWAP_shift5000'] = df.iloc[-5001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift5500'] = df.iloc[-5501,7] / df.iloc[-1,7]\n#             ans['VWAP_shift6000'] = df.iloc[-6001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift7000'] = df.iloc[-7001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift8000'] = df.iloc[-8001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift9000'] = df.iloc[-9001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift10000'] = df.iloc[-10001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift11000'] = df.iloc[-11001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift12000'] = df.iloc[-12001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift14000'] = df.iloc[-14001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift16000'] = df.iloc[-16001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift18000'] = df.iloc[-18001,7] / df.iloc[-1,7]\n#             ans['VWAP_shift25000'] = df.iloc[-25001,7] / df.iloc[-1,7]\n            print (\"1.1elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n            start = time.time()\n#             ans['Volume_shift_sma2000'] = (df['Volume'].rolling(2000).sum() / df[\"Volume\"]).apply(np.log).iloc[-1]\n#             ans['Close_shift70'] = df.iloc[-71,5] / df.iloc[-1,5]\n#             ans['Close_shift280'] = df.iloc[-281,5] / df.iloc[-1,5]\n#             ans['SMA_18000_std'] = df['Close'].rolling(18000).std().shift() / df.iloc[:,5]\n            ans['ROCP'] = talib.ROCP(df.iloc[-182:,5], timeperiod=181).iloc[-1]\n            ans['MOM'] = talib.MOM(df.iloc[-177:,5], timeperiod=176).iloc[-1]\n            ans['MOM_1700'] = talib.MOM(df.iloc[-1761:,5], timeperiod=1760).iloc[-1]\n            ans['RSI'] = talib.RSI(df.iloc[-207:,5], timeperiod=206).iloc[-1]\n            #ans['EMA'] = (talib.EMA(df.iloc[-12:,5], timeperiod=11).iloc[-1] - df.iloc[-1,5]) / df.iloc[-1,5]\n            ans['APO'] = talib.APO(df.iloc[-167:,5], fastperiod=117, slowperiod=166, matype=0).iloc[-1]\n            ans['CMO'] = talib.CMO(df.iloc[-205:,5], timeperiod=204).iloc[-1]\n            macdhist = self.MACD(df.iloc[-610:,5], fastperiod=324, slowperiod=296, signalperiod=272)\n            ans['macdhist0'] = macdhist.iloc[-1]\n            ans['macdhist1'] = macdhist.iloc[-2]\n            ans['macdhist5'] = macdhist.iloc[-6]\n            ans['macdhist15'] = macdhist.iloc[-16]\n            ans['ADX'] = talib.ADX(df.iloc[-362:,3], df.iloc[-362:,4], df.iloc[-362:,5], timeperiod=181).iloc[-1]\n            ans['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(df.iloc[-182:,5], timeperiod=181).iloc[-1]\n            ans['HT_DCPERIOD'] = talib.HT_DCPERIOD(df.iloc[-1000:,5]).iloc[-1]\n            ans['NATR'] = talib.NATR(df.iloc[-182:,3], df.iloc[-182:,4], df.iloc[-182:,5], timeperiod=181).iloc[-1]\n            ans['AROONOSC'] = talib.AROONOSC(df.iloc[-182:,3], df.iloc[-182:,4], timeperiod=181).iloc[-1]\n            aroondown, aroonup = talib.AROON(df.iloc[-182:,3], df.iloc[-182:,4], timeperiod=181)\n            ans['aroondown'] = aroondown.iloc[-1]\n            ans['aroonup'] = aroonup.iloc[-1]\n            ans['PLUS_DI'] = talib.PLUS_DI(df.iloc[-182:,3], df.iloc[-182:,4], df.iloc[-182:,5], timeperiod=181).iloc[-1]\n            upper1, middle,lower1 = talib.BBANDS(df.iloc[-182:,5], timeperiod=181, nbdevup=3, nbdevdn=3, matype=0)\n            ans['BBANDS'] = (upper1.iloc[-2:] - df.iloc[-2:,5]).apply(np.log).iloc[-1]\n\n            ans['CDL2CROWS'] = talib.CDL2CROWS(df.iloc[-1000:,2], df.iloc[-1000:,3], df.iloc[-1000:,4], df.iloc[-1000:,5]).iloc[-1]\n            categorical_features = ['CDL2CROWS']\n            \n            #print(ans.info())################################\n            \n            df = ans.copy()\n            print (\"1.2elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n            \n        start = time.time()\n        \n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df.fillna(method='ffill', inplace=True)\n        df.fillna(0, inplace=True)\n        \n        dfX = df.copy()\n        dfX.drop(categorical_features, axis=1, inplace=True)\n        columns = dfX.columns.tolist()\n        X = dfX.values\n        if not test:\n            del dfX\n            gc.collect()\n        \"\"\"正規化 sklearn\n        scaler = StandardScaler()\n        scaler.fit(X)\n        X = scaler.transform(X)\"\"\"\n        \n        print (\"1.3elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        start = time.time()\n        \n        mean_n = [[ 9.99885373e-01,  3.09758273e-02,  1.00018938e+00, -1.61865578e-01,\n                   9.21656800e-01,  1.65773236e-04,  6.56436859e-01,  1.86611370e-05,\n                   1.00013456e+00, -2.89830662e-04,  9.99977581e-01,  2.04807467e-01,\n                   1.00009365e+00,  2.15764428e-03,  9.05655877e-01, -9.88395078e-05,\n                   1.00009076e+00, -5.51462303e-03,  9.10990142e-01, -1.16363673e+01,\n                   1.00010193e+00, -1.56828965e-02,  9.65466065e-01, -6.98199983e-05,\n                   9.99993611e-01,  9.99986900e-01,  9.99979881e-01,  9.99957910e-01,\n                   9.99942407e-01,  9.99909481e-01,  9.99893581e-01,  9.99877334e-01,\n                   9.99838662e-01,  9.99816700e-01,  9.99788575e-01,  9.99761023e-01,\n                   9.99704426e-01,  9.99676509e-01,  9.99649796e-01,  9.99595992e-01,\n                   9.99569505e-01,  9.99468803e-01,  9.99220890e-01,  9.98908461e-01,\n                   9.98604001e-01,  9.98327831e-01,  9.98046306e-01,  9.97759143e-01,\n                   9.97491716e-01,  9.97218832e-01,  9.96945641e-01,  9.96689312e-01,\n                   9.96208285e-01,  9.95795422e-01,  9.95385685e-01,  9.95037603e-01,\n                   9.94681685e-01,  9.94254386e-01,  9.93227183e-01,  9.92229041e-01,\n                   9.91203069e-01,  9.87183158e-01,  7.96081342e+00,  9.99967919e-01,\n                   9.99854823e-01,  4.95978312e-02,  2.16599492e-04,  2.68893869e+00,\n                   2.84985841e+01,  5.01774243e+01,  5.11677265e-07,  3.75372870e-01,\n                   3.66093910e-01, -1.06804469e-03, -1.06421319e-03, -1.04921838e-03,\n                  -1.01535536e-03,  1.73946113e+01, -6.55988342e+06,  8.84185123e-01,\n                   2.13108282e+01,  7.92234250e-01,  2.13442151e+00,  4.46625790e+01,\n                   4.67970006e+01,  7.52846011e+00,  4.47998655e+00]]\n            \n            \n        std_n = [[1.83716857e-02, 3.89781053e+00, 1.95957894e-02, 1.74106538e+01,\n                  2.69214360e-01, 1.68087136e-02, 4.75186737e-01, 4.54993587e-03,\n                  1.95644498e-02, 1.67338689e-01, 1.63526168e-02, 2.37303288e+01,\n                  2.08237552e-02, 8.69332369e-01, 2.93120932e-01, 2.00583700e-02,\n                  1.75618881e-02, 2.86042554e+00, 3.37676176e-01, 8.20031691e+01,\n                  1.87165419e-02, 4.14204761e+00, 1.84251487e-01, 6.56426652e-03,\n                  5.26436091e-03, 7.40879647e-03, 9.02836838e-03, 1.26595499e-02,\n                  1.45492033e-02, 1.76408183e-02, 1.90177510e-02, 2.02852605e-02,\n                  2.29294077e-02, 2.43220605e-02, 2.59342450e-02, 2.74509028e-02,\n                  3.02354843e-02, 3.15529478e-02, 3.28535234e-02, 3.53239890e-02,\n                  3.65483469e-02, 4.12821836e-02, 5.13174474e-02, 5.86539714e-02,\n                  6.52650303e-02, 7.14069839e-02, 7.69020650e-02, 8.20182956e-02,\n                  8.69482487e-02, 9.15959641e-02, 9.61233061e-02, 1.00670713e-01,\n                  1.09340057e-01, 1.17680116e-01, 1.25375445e-01, 1.33253465e-01,\n                  1.40662143e-01, 1.47272021e-01, 1.58726547e-01, 1.69942399e-01,\n                  1.79984757e-01, 2.12667894e-01, 8.42835655e-01, 1.12048392e-02,\n                  2.18179046e-02, 3.50607903e-02, 1.47249468e-02, 3.38298633e+02,\n                  1.02971586e+03, 3.29244781e+00, 1.76477768e-03, 5.60760509e+01,\n                  6.53106150e+00, 6.53123593e+00, 6.53123373e+00, 6.53122532e+00,\n                  6.53120810e+00, 1.57331750e+01, 4.61154350e+06, 3.59955260e+01,\n                  5.35233709e+00, 1.04905727e+00, 5.59595607e+01, 3.31159638e+01,\n                  3.37248055e+01, 4.47656604e+00, 1.38126899e+00]]\n        \n        mean_n = X.mean(axis = 0, keepdims = True)\n        #print(mean_n)\n        #標準偏差を計算 ddof=0なら標準偏差、ddof=1なら不偏標準偏差\n        std_n = X.std(axis = 0, keepdims = True, ddof = 0)\n        #print(std_n)\n        #標準化の計算\n        X = (X - mean_n) / std_n\n        \n        df = pd.concat([pd.DataFrame(X, columns=columns), df[categorical_features]], axis=1)\n\n        if save_fet:\n            df.to_csv(save_name+'.csv', index=False)\n            \n        if save_mem:\n            df = reduce_mem_usage(df)\n        \n        if not test:\n            del df_list, X\n            gc.collect()\n        print (\"1.4elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n\n        return df\n    \n    def normalize(self, df):\n        return (df - df.mean()) / df.std(ddof=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:25.603241Z","iopub.execute_input":"2022-01-18T04:54:25.603525Z","iopub.status.idle":"2022-01-18T04:54:26.101536Z","shell.execute_reply.started":"2022-01-18T04:54:25.603488Z","shell.execute_reply":"2022-01-18T04:54:26.100624Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Util","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport numpy as np\n\nclass Util():\n\n    def __init__(self) -> None:\n        pass\n\n    def data_conv(self, data):\n        #data = (data > 0.5).astype(int)\n        return data\n\n\n    def accuracy_score(self, train, predict):\n        \"\"\"\n        どのくらい答えに近いか評価するスコアを出す\n        あっているほど数値が高いようにする\n        コンペによって評価方法が違うからこれを変える\n        \"\"\"\n        #ピアソンの相関係数\n        return np.corrcoef(train,predict)[0,1]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:26.104321Z","iopub.execute_input":"2022-01-18T04:54:26.104945Z","iopub.status.idle":"2022-01-18T04:54:26.123447Z","shell.execute_reply.started":"2022-01-18T04:54:26.104898Z","shell.execute_reply":"2022-01-18T04:54:26.122627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# モデル","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import ElasticNet\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import Pool\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport pandas as pd\nimport pickle\n\nfrom memory_profiler import profile\nimport gc\n\n\nut = Util()\n\nclass Models:\n    def __init__(self,ID) -> None:\n        fileID = ID\n        self.models_xgboost = []\n        for fold_id in range(4):\n            try:\n                with open('xgboost'+str(fileID)+'0'+str(fold_id)+'.pickle', 'rb') as web:\n                    self.models_xgboost.append(pickle.load(web))\n                #print('load')\n            except FileNotFoundError:\n                #print('Not find '+str(fileID)+'0'+str(fold_id))\n                pass\n    \n    def __del__(self):\n        \"\"\"オブジェクトが破棄されるとき呼び出される\"\"\"\n        print('Models died:', id(self))\n\n    def select_model(self, categorical_features, model_name, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None):\n        if model_name == \"random_forest\":\n            score, y_val_pre, y_pred = self.random_forest(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"light_gbm\":\n            score, y_val_pre, y_pred = self.light_gbm(categorical_features, learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == 'xgboost':\n            score, y_val_pre, y_pred = self.xgboost(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"catboost\":\n            score, y_val_pre, y_pred = self.catboost(categorical_features, learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"logistic_regression\":\n            score, y_val_pre, y_pred = self.logistic_regression(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"dnn\":\n            score, y_val_pre, y_pred = self.dnn(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        else:\n            raise NameError(\"指定されたアルゴリズムは存在しません\")\n            \n\n        return score, y_val_pre, y_pred\n    \n    def KFold(self, categorical_features, model_name, learn_type, fileID=0, X_train=None, y_train=None, X_test=None, n_splits=2):\n\n        cv_score, oof_pre, y_sub = None, None, None\n        scores = []\n        oof_pre = np.array([])\n        valid_indexs = np.array([])\n        y_preds = []\n        cv = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n        if learn_type=='learn':\n            for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n                X_tr = X_train.loc[train_index, :]\n                X_val = X_train.loc[valid_index, :]\n                y_tr = y_train[train_index]\n                y_val = y_train[valid_index]\n\n                score, y_val_pre, _ = self.select_model(categorical_features, model_name, 'learn', fileID=str(fileID)+str(fold_id),X_train=X_tr, y_train=y_tr, X_valid=X_val, y_valid=y_val)\n\n                scores.append(score)\n                oof_pre = np.append(oof_pre,y_val_pre)\n                valid_indexs = np.append(valid_indexs, valid_index)\n\n            oof_pre = oof_pre[np.argsort(valid_indexs)]\n            cv_score = sum(scores) / len(scores)\n        elif learn_type=='predict':\n            for fold_id in range(n_splits):\n                score, y_val_pre, y_pred = self.select_model(categorical_features, model_name, 'predict', fileID=str(fileID)+str(fold_id), X_test=X_test)\n                oof_pre = np.append(oof_pre,y_val_pre)\n                y_preds.append(y_pred)\n            \n            y_sub = sum(y_preds) / len(y_preds)\n\n        return cv_score, oof_pre, y_sub\n\n    def random_forest(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, n_estimators=67, max_depth=6, random_state=0):\n        \"\"\"\n        pandasでの教師データ\n        パラメータ\n        return valスコア(float)、その取り出し方での予測値\n        \"\"\"\n        print('========random_forest========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('RandomForest'+str(fileID)+'.pickle', 'rb') as web:\n                RandomForest = pickle.load(web)\n            y_pred = RandomForest.predict(X_test)\n        elif learn_type=='learn':\n            #RandomForest = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n            RandomForest = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n            RandomForest.fit(X_train, y_train)\n            y_val_pre = RandomForest.predict(X_valid)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('RandomForest'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(RandomForest , web)\n\n        return score, y_val_pre, y_pred\n\n    def light_gbm(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, params = {'objective': 'binary','max_bin': 284,'learning_rate': 0.068,'num_leaves': 45}):\n        \"\"\"\n        pandasでの教師データ\n        categorical_features:カテゴリかる属性のカラム名を示したリスト\n        パラメータ\n        return valスコア(float), y_val_pre(valでの予測値), その取り出し方での予測値\n        \"\"\"\n        print('========light_gbm========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('light_gbm'+str(fileID)+'.pickle', 'rb') as web:\n                model = pickle.load(web)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\n            lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, categorical_feature=categorical_features)\n            model = lgb.train(params, lgb_train,valid_sets=[lgb_train, lgb_eval],verbose_eval=10,num_boost_round=1000,early_stopping_rounds=10)\n\n            y_val_pre = model.predict(X_valid, num_iteration=model.best_iteration)\n            y_val_pre = ut.data_conv(y_val_pre)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('light_gbm'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(model , web)\n\n        return score, y_val_pre, y_pred\n    \n    def xgboost(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, params = {'tree_method': 'gpu_hist', 'objective': 'reg:squarederror','silent':1, 'random_state':0,'learning_rate': 0.15, 'eval_metric': 'rmse',}, num_round = 450):\n        \"\"\"\n        pandasでの教師データ\n        categorical_features:カテゴリかる属性のカラム名を示したリスト\n        パラメータ\n        return valスコア(float), y_val_pre(valでの予測値), その取り出し方での予測値\n        \"\"\"\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            test = xgb.DMatrix(X_test)\n            model = self.models_xgboost[int(str(fileID)[-1])]\n            y_pred = model.predict(test)\n        elif learn_type=='learn':\n            print('========xgboost========')\n            train = xgb.DMatrix(X_train, label=y_train)\n            valid = xgb.DMatrix(X_valid, label=y_valid)\n            self.model_xgboost = xgb.train(params,\n                    train,#訓練データ\n                    num_round,#設定した学習回数\n                    early_stopping_rounds=20,\n                    evals=[(train, 'train'), (valid, 'eval')],\n                    verbose_eval=100\n                    )\n            y_val_pre = self.model_xgboost.predict(valid)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('xgboost'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(self.model_xgboost, web)\n                \n        \n        \n#         _, ax = plt.subplots(figsize=(12, 15))\n#         xgb.plot_importance(self.model_xgboost,\n#                     ax=ax,\n#                     importance_type='gain',\n#                     show_values=False)\n#         plt.show()\n        \n        \n        return score, y_val_pre, y_pred\n\n    def catboost(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, params ={'depth' : 3,'learning_rate' : 0.054,'early_stopping_rounds' : 9,'iterations' : 474, 'loss_function' : 'RMSE', 'random_seed' :0}):\n        \"\"\"\n        pandasでの教師データ\n        categorical_features:カテゴリかる属性のカラム名を示したリスト\n        パラメータ\n        return valスコア(float), y_val_pre(valでの予測値), その取り出し方での予測値\n        \"\"\"\n        print('========catboost========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('catboost'+str(fileID)+'.pickle', 'rb') as web:\n                model = pickle.load(web)\n            y_pred = model.predict(X_test)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            train = Pool(X_train, y_train, cat_features=categorical_features)\n            eval = Pool(X_valid, y_valid, cat_features=categorical_features)\n            #cab = CatBoostClassifier(custom_loss=['Accuracy'],random_seed=0)\n            #cab = CatBoostClassifier(**params)\n            cab = CatBoostRegressor(random_seed=0)\n            cab = CatBoostRegressor(**params)\n            model = cab.fit(train, eval_set=eval)\n\n            y_val_pre = model.predict(X_valid)\n            y_val_pre = ut.data_conv(y_val_pre)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('catboost'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(model , web)\n\n        return score, y_val_pre, y_pred\n\n    def logistic_regression(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None):\n        \"\"\"\n        pandasでの教師データ\n        パラメータ\n        return valスコア(float)、その取り出し方での予測値\n        \"\"\"\n        print('========logistic_regression========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('logistic_regression'+str(fileID)+'.pickle', 'rb') as web:\n                model = pickle.load(web)\n            y_pred = model.predict(X_test)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            #model = LogisticRegression(penalty='l2', solver='sag', random_state=0)\n            model = ElasticNet(random_state=0)\n            model.fit(X_train, y_train)\n            y_val_pre = model.predict(X_valid)\n            y_val_pre = ut.data_conv(y_val_pre)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('logistic_regression'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(model , web)\n\n        return score, y_val_pre, y_pred\n\n    def dnn(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None):\n        \n        print('========dnn========')\n        score, y_val_pre, y_pred = None, None, None\n\n        lr_schedule=tf.keras.optimizers.schedules.ExponentialDecay( \\\n                    initial_learning_rate=0.001, #初期の学習率\n                    decay_steps=3, #減衰ステップ数\n                    decay_rate=0.01, #最終的な減衰率 \n                    staircase=True)\n\n        model=Sequential()\n        model.add(Dense(len(X_train.columns),input_shape=(len(X_train.columns),),activation='relu',\n                    kernel_regularizer=keras.regularizers.l2(0.001), #重みの正則化考慮\n                    kernel_initializer='random_uniform',\n                    bias_initializer='zero'))\n                    \n        model.add(BatchNormalization()) #バッチ正規化\n        model.add(Dropout(0.1)) # ドロップアウト層・ドロップアウトさせる割合\n        model.add(Dense(int(len(pd.DataFrame(X_train).columns)/2),activation='sigmoid'))\n\n        model.add(BatchNormalization()) #バッチ正規化\n        model.add(Dropout(0.1)) # ドロップアウト層・ドロップアウトさせる割合\n        model.add(Dense(int(len(pd.DataFrame(X_train).columns)/2),activation='sigmoid'))\n\n        model.add(BatchNormalization()) #バッチ正規化\n        model.add(Dropout(0.1)) # ドロップアウト層・ドロップアウトさせる割合\n        model.add(Dense(len(pd.DataFrame(y_train).columns),activation='sigmoid'))\n        Ecall=EarlyStopping(monitor='val_loss',patience=1000,restore_best_weights=False)\n        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule))\n        model.summary()\n\n        if learn_type=='predict':\n            model.load_weights('dnn'+str(fileID)+'.h5')\n            y_pred = model.predict(X_test)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            res=model.fit(X_train.values,y_train.values,epochs=3,callbacks=[Ecall],verbose=1,validation_data=(X_valid.values,y_valid.values))\n            y_val_pre = model.predict(X_valid)[:,0]\n            y_val_pre = ut.data_conv(y_val_pre)\n            print(y_valid.shape)\n            print(y_val_pre.shape)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            model.save_weights('dnn'+str(fileID)+'.h5')\n\n        return score, y_val_pre, y_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:26.124928Z","iopub.execute_input":"2022-01-18T04:54:26.125442Z","iopub.status.idle":"2022-01-18T04:54:29.225137Z","shell.execute_reply.started":"2022-01-18T04:54:26.125394Z","shell.execute_reply":"2022-01-18T04:54:29.224331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# パラメータオプティマイザー","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport optuna\nfrom sklearn.metrics import log_loss\n\n#md = Models(0)\n\n#random_forest : {'n_estimators': 67, 'max_depth': 6}\n#light_gbm : {'max_bin': 284, 'learning_rate': 0.06759289191947715, 'num_leaves': 45}\n#xgboost : {'learning_rate': 0.180343853211702, 'num_round': 394}\n#catboost : {'depth': 3, 'learning_rate': 0.053925065258405916, 'early_stopping_rounds': 9, 'iterations': 474}\nclass Optimizer():\n    def __init__(self) -> None:\n        pass\n\n    def param_opt(self, model_name, X_train, y_train, categorical_features=None):\n        \"\"\"\n        パラメータオプティマイザー\n        model name list: random_forest, light_gbm\n        \"\"\"\n        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3,random_state=0, stratify=y_train)\n        study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n        if model_name == \"random_forest\":\n            study.optimize(self.objective_random_forest(X_train, y_train, X_valid, y_valid), n_trials=80)\n            return study.best_params\n\n        elif model_name == \"light_gbm\":\n            study.optimize(self.objective_light_gbm(X_train, y_train, X_valid, y_valid, categorical_features), n_trials=80)\n            return study.best_params\n\n        elif model_name == \"xgboost\":\n            study.optimize(self.objective_xgboost(X_train, y_train, X_valid, y_valid), n_trials=80)\n            return study.best_params\n\n        elif model_name == \"catboost\":\n            study.optimize(self.objective_catboost(X_train, y_train, X_valid, y_valid, categorical_features), n_trials=80)\n            return study.best_params\n            \n        elif model_name == 'logistic_regression':\n            raise NameError('logistic_regressionはパラメータが存在しないのでサポートしていません')\n\n    def objective_random_forest(self, X_train, y_train, X_valid, y_valid):\n        def objective(trial):\n            n_estimators = trial.suggest_int('n_estimators', 10, 300)\n            max_depth = trial.suggest_int('max_depth', 1, 15)\n            _, y_val_pre, _ = md.random_forest('learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    def objective_light_gbm(self, X_train, y_train, X_valid, y_valid, categorical_features):\n        def objective(trial):\n            params = {\n            'objective': 'binary',\n            'max_bin': trial.suggest_int('max_bin', 255, 500),\n            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n            'num_leaves': trial.suggest_int('num_leaves', 32, 128),\n            }\n            _, y_val_pre, _ = md.light_gbm(categorical_features, 'learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, params=params)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    def objective_xgboost(self, X_train, y_train, X_valid, y_valid):\n        def objective(trial):\n            params = {'objective': 'reg:squarederror',\n                    'silent':1, \n                    'random_state':0,\n                    'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.2), \n                    'eval_metric': 'rmse',\n            }\n            num_round = trial.suggest_int('num_round', 100, 900)\n            _, y_val_pre, _ = md.xgboost('learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, params=params, num_round=num_round)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    def objective_catboost(self, X_train, y_train, X_valid, y_valid, categorical_features):\n        def objective(trial):\n            params = {\n                'depth' : trial.suggest_int('depth', 1, 15),                  # 木の深さ\n                'learning_rate' : trial.suggest_uniform('learning_rate', 0.01, 0.1),       # 学習率\n                'early_stopping_rounds' : trial.suggest_int('early_stopping_rounds', 3, 20),\n                'iterations' : trial.suggest_int('iterations', 50, 500), \n                'custom_loss' :['Accuracy'], \n                'random_seed' :0\n            }\n            _, y_val_pre, _ = md.catboost(categorical_features, 'learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, params=params)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    #LogisticRegressionはパラメータがない","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:29.226769Z","iopub.execute_input":"2022-01-18T04:54:29.227063Z","iopub.status.idle":"2022-01-18T04:54:29.250783Z","shell.execute_reply.started":"2022-01-18T04:54:29.227026Z","shell.execute_reply":"2022-01-18T04:54:29.250014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# アンサンブル","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\nimport numpy as np\nimport pandas as pd\n\n#md = Models()\nut = Util()\n\nclass Ensemble():\n\n    def __init__(self) -> None:\n        pass\n\n    def stacking(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_test=None, fst_lay=['random_forest', 'light_gbm', 'xgboost', 'catboost'], snd_lay='light_gbm', enable_2ndorigx=True):\n        #enable_2ndorigx:二層目にオリジナルの入力データを入力するか\n\n        stack_oof_pred = []\n        stack_pred = []\n        for index, model_name in enumerate(fst_lay):\n            \n            if learn_type=='learn':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_train=X_train, y_train=y_train)\n                stack_oof_pred = oof_pre if index == 0 else np.c_[stack_oof_pred, oof_pre]\n            elif learn_type=='predict':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_test=X_test)\n                stack_pred = y_sub if index == 0 else np.c_[stack_pred, y_sub]\n            else:\n                raise NameError(\"指定されたlearn_typeは存在しません\")\n\n        if enable_2ndorigx:\n            X_train2 =  pd.concat([pd.DataFrame(stack_oof_pred), X_train], axis=1)\n            X_test2 =  pd.concat([pd.DataFrame(stack_pred), X_test], axis=1)\n        else:\n            X_train2 = pd.DataFrame(stack_oof_pred)\n            X_test2 = pd.DataFrame(stack_pred)\n            categorical_features = []\n\n        #二層目\n        if learn_type=='learn':\n            cv_score, oof_pre, y_sub = md.KFold(categorical_features, snd_lay, learn_type, fileID=fileID+'1', X_train=X_train2, y_train=y_train)\n        elif learn_type=='predict':\n            cv_score, oof_pre, y_sub = md.KFold(categorical_features, snd_lay, learn_type, fileID=fileID+'1', X_test=X_test2)\n            y_sub = ut.data_conv(y_sub)\n\n        return cv_score, oof_pre, y_sub\n\n    def mean(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_test=None, models=['random_forest', 'light_gbm', 'xgboost', 'catboost'], type='mean'):\n        '''\n        type:平均の取り方 \n        mean -> 算術平均\n        hmean -> 調和平均\n        gmean -> 幾何平均\n        '''\n\n        stack_oof_pred = []\n        stack_pred = []\n        for index, model_name in enumerate(models):\n            if learn_type=='learn':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_train=X_train, y_train=y_train)\n                stack_oof_pred = oof_pre if index == 0 else np.c_[stack_oof_pred, oof_pre]\n            elif learn_type=='predict':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_test=X_test)\n                stack_pred = y_sub if index == 0 else np.c_[stack_pred, y_sub]\n            else:\n                raise NameError(\"指定されたlearn_typeは存在しません\")\n\n        if type == 'mean':\n            y_off = np.average(stack_oof_pred, axis=1)\n            y_sub = np.average(stack_pred, axis=1)\n        elif type == 'hmean':\n            from scipy.stats import hmean\n            y_off = hmean(stack_oof_pred, axis = 1)\n            y_sub = hmean(stack_pred, axis = 1)\n        elif type == 'gmean':\n            from scipy.stats.mstats import gmean\n            y_off = gmean(stack_oof_pred, axis = 1)\n            y_sub = gmean(stack_pred, axis = 1)\n        \n        y_off = ut.data_conv(y_off)\n        y_sub = ut.data_conv(y_sub)\n\n        cv_score = ut.accuracy_score(y_train, y_off)\n        \n        return cv_score, oof_pre, y_sub","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:29.25245Z","iopub.execute_input":"2022-01-18T04:54:29.252748Z","iopub.status.idle":"2022-01-18T04:54:29.27672Z","shell.execute_reply.started":"2022-01-18T04:54:29.252707Z","shell.execute_reply":"2022-01-18T04:54:29.275904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# コントローラー","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nop = Optimizer()\nens = Ensemble()\n\nclass Controller():\n\n    def __init__(self,ID) -> None:\n        self.md = Models(ID)\n        self.ID = str(ID)\n\n    def opt(self, X_train, y_train):\n        print(op.param_opt('light_gbm', X_train, y_train))\n\n    def KFold_learn(self, categorical_features, X_train, y_train, model_name):\n        cv_score, y_val_pre, y_sub = self.md.KFold(categorical_features, model_name, 'learn', fileID=self.ID+'0', X_train=X_train, y_train=y_train)\n\n        print('CV score-----------------------------------',cv_score)\n        #random_forest 0.822635113928818\n        #light_gbm 0.8293829640323895\n        #catboost 0.8204004770573097\n        #logistic_regression 0.6846023476241291\n        #xgboost 0.8192643274119641\n        #dnn 0.8159060950348378s\n        \n        return cv_score\n\n    def KFold_predict(self, categorical_features, X_test, model_name):\n        cv_score, y_val_pre, y_sub = self.md.KFold(categorical_features, model_name, 'predict', fileID=self.ID+'0', X_test=X_test)\n        \n        return y_sub\n\n    def stacking_learn(self, categorical_features, X_train, y_train, fst_lay=['random_forest', 'light_gbm', 'xgboost', 'catboost'], snd_lay='light_gbm', enable_2ndorigx=False):\n        cv_score, _, y_sub = ens.stacking(categorical_features, 'learn', fileID=self.ID, X_train=X_train, y_train=y_train, fst_lay=fst_lay, snd_lay=snd_lay, enable_2ndorigx=enable_2ndorigx)\n\n        print('CV score-----------------------------------',cv_score)\n        #0.8293829640323895 2ndlgtm\n        #0.8237712635741635 2ndrandomforest\n\n    def stacking_predict(self, categorical_features, X_test, fst_lay=['random_forest', 'light_gbm', 'xgboost', 'catboost'], snd_lay='light_gbm', enable_2ndorigx=False):\n        _, _, y_sub = ens.stacking(categorical_features, 'predict', fileID=self.ID, X_test=X_test, fst_lay=fst_lay, snd_lay=snd_lay, enable_2ndorigx=enable_2ndorigx)\n\n        sub = pd.read_csv('input/titanic/gender_submission.csv')\n        sub['Survived'] = y_sub\n        sub.to_csv('submission.csv', index=False)\n        #0.8293829640323895\n\n    def mean_learn(self, categorical_features, learn_type, X_train, y_train, models=['random_forest', 'light_gbm', 'xgboost', 'catboost'], type='mean'):\n        cv_score, _, y_sub = ens.mean(categorical_features, learn_type, fileID=self.ID, X_train=X_train, y_train=y_train, models=models, type=type)\n        #mean:0.8237934904601572 hmean:0.819304152637486 gmean:0.819304152637486\n        print('CV score-----------------------------------',cv_score)\n\n    def mean_predict(self, categorical_features, learn_type, X_test, models=['random_forest', 'light_gbm', 'xgboost', 'catboost'], type='mean'):\n        cv_score, _, y_sub = ens.mean(categorical_features, learn_type, fileID=self.ID, X_test=X_test, models=models, type=type)\n\n        sub = pd.read_csv('input/titanic/gender_submission.csv')\n        sub['Survived'] = y_sub\n        sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:29.279127Z","iopub.execute_input":"2022-01-18T04:54:29.279658Z","iopub.status.idle":"2022-01-18T04:54:29.296304Z","shell.execute_reply.started":"2022-01-18T04:54:29.279619Z","shell.execute_reply":"2022-01-18T04:54:29.29548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"markdown","source":"教師データ読み込み","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\nseed_everything\n\ndata_folder = \"../input/g-research-crypto-forecasting/\"\n#crypto_df = reduce_mem_usage(pd.read_csv(data_folder + 'train.csv'))\ncrypto_df = pd.read_csv(data_folder + 'train.csv')\n\ntrain_list = list(range(13))\nfor Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n    #train = reduce_mem_usage(crypto_df[crypto_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\"))\n    train = crypto_df[crypto_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n    train_list[Asset_ID] = train\ndel crypto_df\ndel train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:54:29.298128Z","iopub.execute_input":"2022-01-18T04:54:29.298618Z","iopub.status.idle":"2022-01-18T04:55:05.592443Z","shell.execute_reply.started":"2022-01-18T04:54:29.298577Z","shell.execute_reply":"2022-01-18T04:55:05.59166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 学習","metadata":{}},{"cell_type":"code","source":"scores = []\nfe = Feature()\nfor Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n    \n    print('通貨番号',Asset_ID)\n    \n    train_raw = train_list[Asset_ID].copy()\n    y_train = train_raw['Target'].copy()\n    X_train = train_raw.drop('Target', axis=1).copy()\n    \n    y_train.reset_index(drop=True, inplace=True)\n    y_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n    y_train.fillna(method='ffill', inplace=True)\n    y_train.fillna(0, inplace=True)\n    X_train = fe.conv_data(X_train, Asset_ID, train_list, save_fet=False, save_name='feature')\n    #print(train)\n    #print(train_list[1])\n    categorical_features = ['CDL2CROWS']\n    \n    ct = Controller(Asset_ID)\n            \n    cv_score = ct.KFold_learn(categorical_features, X_train, y_train, 'xgboost')\n    scores.append(cv_score)\n    #ct.stacking_learn(categorical_features, X_train, y_train, fst_lay=['dnn', 'light_gbm', 'xgboost'], snd_lay='light_gbm', enable_2ndorigx=False)\n    \n    del train_raw, y_train, X_train, ct\n    gc.collect()\n    \n    print('5',psutil.virtual_memory().percent)\n    \n    \nprint('CV mean-----------------------------'+str(sum(scores)/len(scores)))\n\n#CV score----------------------------------- 0.23738583123946141 light_gbm 0.2578\n#xgboost 0.44045052797107087\n#random forest 微妙　0.15 \n#catboost 0.06","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:55:05.594029Z","iopub.execute_input":"2022-01-18T04:55:05.594298Z","iopub.status.idle":"2022-01-18T04:59:50.981351Z","shell.execute_reply.started":"2022-01-18T04:55:05.594261Z","shell.execute_reply":"2022-01-18T04:59:50.980614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #del train_raw\n# del X_train, y_train\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:50.982768Z","iopub.execute_input":"2022-01-18T04:59:50.983027Z","iopub.status.idle":"2022-01-18T04:59:50.986569Z","shell.execute_reply.started":"2022-01-18T04:59:50.982992Z","shell.execute_reply":"2022-01-18T04:59:50.985486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del train_list\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:50.990845Z","iopub.execute_input":"2022-01-18T04:59:50.991329Z","iopub.status.idle":"2022-01-18T04:59:50.99763Z","shell.execute_reply.started":"2022-01-18T04:59:50.991286Z","shell.execute_reply":"2022-01-18T04:59:50.996839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sys\n\n# print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n# print(\" ------------------------------------ \")\n# for var_name in dir():\n#     if not var_name.startswith(\"_\"):\n#         print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:50.998923Z","iopub.execute_input":"2022-01-18T04:59:50.999227Z","iopub.status.idle":"2022-01-18T04:59:51.005741Z","shell.execute_reply.started":"2022-01-18T04:59:50.999192Z","shell.execute_reply":"2022-01-18T04:59:51.004996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_list[0].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:51.007064Z","iopub.execute_input":"2022-01-18T04:59:51.007322Z","iopub.status.idle":"2022-01-18T04:59:51.014691Z","shell.execute_reply.started":"2022-01-18T04:59:51.007287Z","shell.execute_reply":"2022-01-18T04:59:51.013941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fe.conv_data(train_list[0], save_fet=False, save_name='feature')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:51.016108Z","iopub.execute_input":"2022-01-18T04:59:51.016389Z","iopub.status.idle":"2022-01-18T04:59:51.022745Z","shell.execute_reply.started":"2022-01-18T04:59:51.016336Z","shell.execute_reply":"2022-01-18T04:59:51.021848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## predict","metadata":{}},{"cell_type":"code","source":"supplemental_df = pd.read_csv(data_folder + 'supplemental_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:51.025316Z","iopub.execute_input":"2022-01-18T04:59:51.025601Z","iopub.status.idle":"2022-01-18T04:59:53.795603Z","shell.execute_reply.started":"2022-01-18T04:59:51.025573Z","shell.execute_reply":"2022-01-18T04:59:53.794841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_list = list(range(13))\nfor Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n    supplemental_train = supplemental_df[supplemental_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n    if len(supplemental_train) > 1000:#30000\n        #supplemental_df.drop(supplemental_df.index[-26000:], inplace=True)\n        supplemental_train = supplemental_train.iloc[-1000:,:]\n    test_list[Asset_ID] = supplemental_train","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:53.797095Z","iopub.execute_input":"2022-01-18T04:59:53.797343Z","iopub.status.idle":"2022-01-18T04:59:54.120676Z","shell.execute_reply.started":"2022-01-18T04:59:53.797309Z","shell.execute_reply":"2022-01-18T04:59:54.119912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gresearch_crypto\n#以下二つは1セッションで一度しか実行できない\nenv = gresearch_crypto.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:54.122246Z","iopub.execute_input":"2022-01-18T04:59:54.122517Z","iopub.status.idle":"2022-01-18T04:59:54.133459Z","shell.execute_reply.started":"2022-01-18T04:59:54.122481Z","shell.execute_reply":"2022-01-18T04:59:54.132733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfe = Feature()\nctlist = []\ncategorical_features = ['CDL2CROWS']\nfor Asset_ID in range(13):\n    ct = Controller(Asset_ID)\n    ctlist.append(ct)\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    start1 = time.time()\n    start = time.time()\n    for Asset_ID in range(13):\n        test_raw = df_test[df_test[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n        test = pd.concat([test_list[Asset_ID], test_raw], sort=False)\n        if len(test) > 1000:#30000\n            #test.drop(test.index[-26000:], inplace=True)\n            test = test.iloc[-1000:,:]\n        test_list[Asset_ID] = test.copy()\n    print (\"1elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n    for Asset_ID in range(13):\n        start = time.time()\n        print('通貨番号',Asset_ID)\n\n        test_raw = test_list[Asset_ID].copy()\n        \n        #print(test_raw)\n        \n        row_id = test_raw.iat[-1, 9]\n        print('row_id :',row_id)\n        X_test = test_raw.drop(['Target','row_id'], axis=1).copy()\n        print (\"2elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        start = time.time()\n        X_test = fe.conv_data(X_test, Asset_ID, test_list, save_fet=False, save_name='feature', save_mem=False, test=True)\n        print (\"3elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        start = time.time()\n        ct = ctlist[Asset_ID]\n        y_sub = ct.KFold_predict(categorical_features, X_test, 'xgboost')[-1]\n        #y_sub = ct.stacking_predict(categorical_features, X_test)\n        print (\"4elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        \n        print('pred :',y_sub)\n        #print(df_pred)\n        #print(df_pred['row_id'] == row_id)\n        \n        df_pred.loc[df_pred['row_id'] == row_id, 'Target'] = y_sub\n    \n    #print(df_pred)\n    print (\"elapsed_time:{0}\".format(time.time() - start1) + \"[sec]\")\n    env.predict(df_pred)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:54.134709Z","iopub.execute_input":"2022-01-18T04:59:54.135477Z","iopub.status.idle":"2022-01-18T04:59:59.694051Z","shell.execute_reply.started":"2022-01-18T04:59:54.135438Z","shell.execute_reply":"2022-01-18T04:59:59.69283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe = Feature()\nX_test = test_raw.drop('Target', axis=1).copy()\n\nprint(1)\nX_test = fe.conv_data(X_test, Asset_ID, test_list, save_fet=False, save_name='feature', save_mem=False).iloc[-1:]\nprint(2)\ny_sub = ct.KFold_predict(categorical_features, X_test, 'xgboost')\nprint(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.695274Z","iopub.status.idle":"2022-01-18T04:59:59.695894Z","shell.execute_reply.started":"2022-01-18T04:59:59.695653Z","shell.execute_reply":"2022-01-18T04:59:59.695679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub[-1]","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.697141Z","iopub.status.idle":"2022-01-18T04:59:59.697756Z","shell.execute_reply.started":"2022-01-18T04:59:59.697522Z","shell.execute_reply":"2022-01-18T04:59:59.697547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# G-Research\n\n* tutrial - [https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition](http://)\n\n# description\n\n* 15分後の価格の変動率を予測する\n* 常に市場の傾向が変動するので、確実な予測モデルを立てるのが難しい(傾向が非定常的)\n* オーバーフィッティングの可能性が高い\n* 通貨間での価格変動の関連性がある。特にビットコインはほかの通貨に影響を与えやすい。\n* 将来、価格がどのように動くかを予測することである。過去の価格の時系列データを学習データとして、価格が上がるか下がるか、またどの程度上がるか、すなわち資産リターンを予測\n\n### Data\n\n#### train.csv\n\n* timestamp: データのUNIX秒。すべて60秒間隔よって、一分ごとにデータが与えられている。\n* Asset_ID: asset_details.csvに書いている通貨IDと結びついており、通貨の種類の識別に使う。 (e.g. Asset_ID = 1 for Bitcoin)\n* Count: 前の一分間で取引された回数\n* Open: 始値 (in USD).\n* High: 前の一分間での最大の価格 (in USD).\n* Low: 前の一分間での最小の価格 (in USD).\n* Close: Close price 終値 (in USD).\n* Volume: 前の一分間の引通貨量(USD)\n* VWAP: 一定期間内での取引価格の、取引量による加重平均\n* Target: 15分前の価格との差をlogでとったもの Residual log-returns for the asset over a 15 minute horizon.\n\n#### asset_details.csv\n\n* Asset_ID 通貨ID\n* Weight 性能評価するときにその通貨の正答率がどのくらい加味されるかの重み\n* Asset_Name IDに結びついている通貨名\n\n\n### 評価方法\n\n\n\n\n\n# task\n\n1. 概要(overview)をしっかり読む\n2. 似ている過去のコンペを探し、参加し基本的な分析を行う\n3. 似たような大会のsolutionを読む\n4. 論文を読んでその分野の進捗を見逃さないようにする\n5. データを分析し安定したCVのモデルを構築する\n6. データ前処理、特徴量エンジニアリングを行い一定のモデルでCVを比較しいい特徴量エンジニアリングを探す\n7. モデルの予測と教師データを比較し分析、予測の難しいデータに対し考察\n8. 分析に基づき高性能なモデルをアンサンブルなどを取り入れて構築\n9.  データ解析、結果分析からより高度な予測の難しいサンプルを解決するモデルを設計\n10. 必要であれば前のステップに戻る\n\n# scores\n\n# else\n\n* supplemental_trainとtrainをくっつけて最後に学習\n* B (billion)\t1,000,000,000","metadata":{}},{"cell_type":"markdown","source":"# 可視化","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.698921Z","iopub.status.idle":"2022-01-18T04:59:59.699555Z","shell.execute_reply.started":"2022-01-18T04:59:59.69932Z","shell.execute_reply":"2022-01-18T04:59:59.699345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_folder = \"../input/g-research-crypto-forecasting/\"\n\n# crypto_df = reduce_mem_usage(pd.read_csv(data_folder + 'train.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.70071Z","iopub.status.idle":"2022-01-18T04:59:59.70134Z","shell.execute_reply.started":"2022-01-18T04:59:59.701095Z","shell.execute_reply":"2022-01-18T04:59:59.70112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_list = []\n# for Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n#     train = crypto_df[crypto_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n#     train = fe.conv_data(train, save_fet=False, save_name='feature')\n#     train_list.append(train)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.702528Z","iopub.status.idle":"2022-01-18T04:59:59.703147Z","shell.execute_reply.started":"2022-01-18T04:59:59.702901Z","shell.execute_reply":"2022-01-18T04:59:59.702925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fe = Feature()\n# for Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n#     train = train_list[Asset_ID]\n    \n#     categorical_features = ['Embarked', 'Pclass', 'Sex']\n#     y_train = train['Target']\n#     X_train = train.drop('Target', axis=1)\n    \n#     #ct = Controller(Asset_ID)\n#     #ct.stacking_learn(categorical_features, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.704341Z","iopub.status.idle":"2022-01-18T04:59:59.704939Z","shell.execute_reply.started":"2022-01-18T04:59:59.704703Z","shell.execute_reply":"2022-01-18T04:59:59.704728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gresearch_crypto\n# #以下二つは1セッションで一度しか実行できない\n# env = gresearch_crypto.make_env()   # initialize the environment\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.706132Z","iopub.status.idle":"2022-01-18T04:59:59.706742Z","shell.execute_reply.started":"2022-01-18T04:59:59.706507Z","shell.execute_reply":"2022-01-18T04:59:59.706532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for (test_df, sample_prediction_df) in iter_test:\n    \n#     for Asset_ID in df_test.Asset_ID.unique():\n#         df_test = df_test[df_test[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n        \n#         test = fe.conv_data(test, save_fet=False, save_name='feature')\n\n#         categorical_features = ['Embarked', 'Pclass', 'Sex']\n#         y_train = train['Target']\n        \n#         ct = Controller(Asset_ID)\n#         ct.stacking_predict(categorical_features, X_test)\n    \n#     #testと教師データを結合して特徴量を作る\n    \n#     sample_prediction_df['Target'] = 0  # make your predictions here\n#     env.predict(sample_prediction_df)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.707929Z","iopub.status.idle":"2022-01-18T04:59:59.708561Z","shell.execute_reply.started":"2022-01-18T04:59:59.708328Z","shell.execute_reply":"2022-01-18T04:59:59.708353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for (test_df, sample_prediction_df) in iter_test:\n#     sample_prediction_df['Target'] = 0\n#     env.predict(sample_prediction_df)\n#     print(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.709931Z","iopub.status.idle":"2022-01-18T04:59:59.710643Z","shell.execute_reply.started":"2022-01-18T04:59:59.710373Z","shell.execute_reply":"2022-01-18T04:59:59.710402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_folder = \"../input/g-research-crypto-forecasting/\"\n# crypto_df = pd.read_csv(data_folder + 'supplemental_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.712024Z","iopub.status.idle":"2022-01-18T04:59:59.712627Z","shell.execute_reply.started":"2022-01-18T04:59:59.712393Z","shell.execute_reply":"2022-01-18T04:59:59.712417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas_profiling\n\n# train_list[0].profile_report()","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.713812Z","iopub.status.idle":"2022-01-18T04:59:59.714443Z","shell.execute_reply.started":"2022-01-18T04:59:59.714201Z","shell.execute_reply":"2022-01-18T04:59:59.714234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# feature optimizer","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# data_folder = \"../input/g-research-crypto-forecasting/\"\n# crypto_df = pd.read_csv(data_folder + 'train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.71559Z","iopub.status.idle":"2022-01-18T04:59:59.716197Z","shell.execute_reply.started":"2022-01-18T04:59:59.71596Z","shell.execute_reply":"2022-01-18T04:59:59.715985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import optuna\n# import talib\n\n# price = np.array(crypto_df['Close'])\n# returns = np.array(crypto_df['Close'].shift(-15)) - price\n# #returns = np.array(crypto_df['Target'])\n\n# #ROCP {'timeperiod': 6} best param -0.0005372071418046449 best score\n# #MOM {'timeperiod': 127} best param -0.001971999282000135 best score\n# #RSI -0.0009095762807234701 3\n# #EMA {'timeperiod': 11} best param -0.00034526380519110374 best score\n\n# def objective(trial):\n#     timeperiod = trial.suggest_int('timeperiod', 2, 240)\n    \n#     df = talib.ROCP(price, timeperiod=timeperiod)\n#     returns_new = returns[~np.isnan(df)]\n#     df_new = df[~np.isnan(df)]\n#     returns_last = returns_new[~np.isnan(returns_new)]\n#     df_new = df_new[~np.isnan(returns_new)]\n#     ic = get_ic(df_new, returns_last)\n#     print(ic, timeperiod)\n#     return -abs(ic)\n\n# def get_ic(x, returns, normalize=True) -> float:\n#     \"\"\"\n#     :param np.ndarray x: 指標\n#     :param np.ndarray returns: リターン\n#     :param bool normalize: x をスケーリングするかどうか\n#     \"\"\"\n#     assert(len(x) == len(returns))\n#     x = (x - x.mean()) / x.std() if normalize else x\n#     returns = (returns - returns.mean()) / returns.std() if normalize else returns\n#     ic = np.corrcoef(x, returns)[0, 1]\n\n#     return ic\n\n# study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n# study.optimize(objective, n_trials=100)\n# print(study.best_params,'best param')\n# print(study.best_value,'best score')","metadata":{"execution":{"iopub.status.busy":"2022-01-18T04:59:59.717416Z","iopub.status.idle":"2022-01-18T04:59:59.718031Z","shell.execute_reply.started":"2022-01-18T04:59:59.717779Z","shell.execute_reply":"2022-01-18T04:59:59.717804Z"},"trusted":true},"execution_count":null,"outputs":[]}]}