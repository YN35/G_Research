{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# install talib","metadata":{}},{"cell_type":"code","source":"!cp ../input/talibinstall/ta-lib-0.4.0-src.tar.gzh  ./ta-lib-0.4.0-src.tar.gz\n!tar -xzvf ta-lib-0.4.0-src.tar.gz > null\n!cd ta-lib && ./configure --prefix=/usr > null && make  > null && make install > null\n!cp ../input/talibinstall/TA-Lib-0.4.21.tar.gzh TA-Lib-0.4.21.tar.gz\n!pip install TA-Lib-0.4.21.tar.gz > null\n!pip install ../input/talibinstall/numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl >null\nimport talib\ntalib.__version__","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:40:46.778690Z","iopub.execute_input":"2022-01-23T05:40:46.779688Z","iopub.status.idle":"2022-01-23T05:44:47.364182Z","shell.execute_reply.started":"2022-01-23T05:40:46.779529Z","shell.execute_reply":"2022-01-23T05:44:47.362554Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"./configure: line 4349: /usr/bin/file: No such file or directory\nlibtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\nar: `u' modifier ignored since `D' is the default (see `U')\nlibtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\nar: `u' modifier ignored since `D' is the default (see `U')\nlibtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\nar: `u' modifier ignored since `D' is the default (see `U')\nlibtool: link: warning: `-version-info/-version-number' is ignored for convenience libraries\nar: `u' modifier ignored since `D' is the default (see `U')\nar: `u' modifier ignored since `D' is the default (see `U')\n\u001b[01m\u001b[Kgen_code.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KprintFuncHeaderDoc\u001b[m\u001b[K’:\n\u001b[01m\u001b[Kgen_code.c:3456:4:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kformat not a string literal and no format arguments [\u001b[01;35m\u001b[K-Wformat-security\u001b[m\u001b[K]\n 3456 |    \u001b[01;35m\u001b[Kfprintf\u001b[m\u001b[K( out, prefix );\n      |    \u001b[01;35m\u001b[K^~~~~~~\u001b[m\u001b[K\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\nexplainable-ai-sdk 1.3.2 requires xai-image-widget, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\ncudf 21.10.1 requires cupy-cuda110, which is not installed.\nbeatrix-jupyterlab 3.1.4 requires google-cloud-bigquery-storage, which is not installed.\nyellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\ntfx-bsl 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntfx-bsl 1.4.0 requires numpy<1.20,>=1.16, but you have numpy 1.21.4 which is incompatible.\ntfx-bsl 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.0 which is incompatible.\ntensorflow 2.6.2 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\ntensorflow 2.6.2 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\ntensorflow 2.6.2 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\ntensorflow 2.6.2 requires wrapt~=1.12.1, but you have wrapt 1.13.3 which is incompatible.\ntensorflow-transform 1.4.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.15.0 which is incompatible.\ntensorflow-transform 1.4.0 requires numpy<1.20,>=1.16, but you have numpy 1.21.4 which is incompatible.\ntensorflow-transform 1.4.0 requires pyarrow<6,>=1, but you have pyarrow 6.0.0 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.0 which is incompatible.\nnumba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.21.4 which is incompatible.\nimbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2021.11.2 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2021.11.2 which is incompatible.\napache-beam 2.34.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.4 which is incompatible.\napache-beam 2.34.0 requires httplib2<0.20.0,>=0.8, but you have httplib2 0.20.2 which is incompatible.\napache-beam 2.34.0 requires numpy<1.21.0,>=1.14.3, but you have numpy 1.21.4 which is incompatible.\napache-beam 2.34.0 requires pyarrow<6.0.0,>=0.15.1, but you have pyarrow 6.0.0 which is incompatible.\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"'0.4.21'"},"metadata":{}}]},{"cell_type":"markdown","source":"### メモリ削減","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:44:47.367393Z","iopub.execute_input":"2022-01-23T05:44:47.367736Z","iopub.status.idle":"2022-01-23T05:44:47.383408Z","shell.execute_reply.started":"2022-01-23T05:44:47.367672Z","shell.execute_reply":"2022-01-23T05:44:47.381980Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import psutil","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:44:47.385076Z","iopub.execute_input":"2022-01-23T05:44:47.385399Z","iopub.status.idle":"2022-01-23T05:44:47.400044Z","shell.execute_reply.started":"2022-01-23T05:44:47.385355Z","shell.execute_reply":"2022-01-23T05:44:47.398893Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 特徴量エンジニアリング","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport talib\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nimport time\nfrom concurrent import futures\n\nclass Feature():\n\n    def __init__(self) -> None:\n        pass\n    \n    def __del__(self):\n        \"\"\"オブジェクトが破棄されるとき呼び出される\"\"\"\n        print('Feature died:', id(self))\n    def MACD(self, close : pd.DataFrame, fastperiod=324, slowperiod=296, signalperiod=272):\n        exp1 = close.rolling(fastperiod).mean()\n        exp2 = close.rolling(slowperiod).mean()\n        macd = 100 * (exp1 - exp2) / exp2\n        signal = macd.rolling(signalperiod).mean()\n        hist = signal - macd\n\n        return hist\n    \n    def task(self, num, df):\n        if num == 0:\n            return df.iloc[-16,7] / df.iloc[-1,7]\n        elif num == 1:\n            return df.iloc[-46,7] / df.iloc[-1,7]\n        elif num == 2:\n            return df.iloc[-91,7] / df.iloc[-1,7]\n        elif num == 3:\n            return df.iloc[-181,7] / df.iloc[-1,7]\n        elif num == 4:\n            return df.iloc[-211,7] / df.iloc[-1,7]\n        elif num == 5:\n            return df.iloc[-311,7] / df.iloc[-1,7]\n        elif num == 6:\n            return df.iloc[-351,7] / df.iloc[-1,7]\n        elif num == 7:\n            return df.iloc[-451,7] / df.iloc[-1,7]\n        elif num == 8:\n            return talib.ROCP(df.iloc[-182:,5], timeperiod=181).iloc[-1]\n        elif num == 9:\n            return talib.MOM(df.iloc[-177:,5], timeperiod=176).iloc[-1]\n        elif num == 10:\n            return talib.RSI(df.iloc[-207:,5], timeperiod=206).iloc[-1]\n        elif num == 11:\n            macdhist = self.MACD(df.iloc[-610:,5], fastperiod=324, slowperiod=296, signalperiod=272)\n            return macdhist.iloc[-1]\n        elif num == 12:\n            macdhist = self.MACD(df.iloc[-610:,5], fastperiod=324, slowperiod=296, signalperiod=272)\n            return macdhist.iloc[-2]\n        elif num == 13:\n            macdhist = self.MACD(df.iloc[-610:,5], fastperiod=324, slowperiod=296, signalperiod=272)\n            return macdhist.iloc[-6]\n        elif num == 14:\n            macdhist = self.MACD(df.iloc[-610:,5], fastperiod=324, slowperiod=296, signalperiod=272)\n            return macdhist.iloc[-16]\n        elif num == 15:\n            return talib.LINEARREG_ANGLE(df.iloc[-182:,5], timeperiod=181).iloc[-1]\n        elif num == 16:\n            return talib.HT_DCPERIOD(df.iloc[-1000:,5]).iloc[-1]\n        elif num == 17:\n            return talib.AROONOSC(df.iloc[-182:,3], df.iloc[-182:,4], timeperiod=181).iloc[-1]\n        elif num == 18:\n            aroondown, aroonup = talib.AROON(df.iloc[-182:,3], df.iloc[-182:,4], timeperiod=181)\n            return aroondown.iloc[-1]\n        elif num == 19:\n            aroondown, aroonup = talib.AROON(df.iloc[-182:,3], df.iloc[-182:,4], timeperiod=181)\n            return aroonup.iloc[-1]\n        elif num == 20:\n            return talib.PLUS_DI(df.iloc[-182:,3], df.iloc[-182:,4], df.iloc[-182:,5], timeperiod=181).iloc[-1]\n        elif num == 21:\n            upper1, middle,lower1 = talib.BBANDS(df.iloc[-182:,5], timeperiod=181, nbdevup=3, nbdevdn=3, matype=0)\n            return (upper1.iloc[-2:] - df.iloc[-2:,5]).apply(np.log).iloc[-1]\n            \n\n    def conv_data(self, df, Asset_ID, df_list, save_fet=False, save_name='feature', save_mem=True, test=False):\n        \n        df.fillna(method='ffill', inplace=True)\n        \n        if not test:\n        \n            for i in range(13):\n                if not (i == Asset_ID and i == 0 and i == 7):\n                    as_df = df_list[i].copy()\n                    #df[str(i)+'VWAP_shift105'] = as_df['VWAP'].shift(105) / as_df[\"VWAP\"]\n                    #df[str(i)+'MOM'] = talib.MOM(as_df['Close'], timeperiod=176)\n\n                    del as_df\n                    gc.collect()\n\n            df['VWAP_shift15'] = df['VWAP'].shift(15) / df[\"VWAP\"]\n            #df['VWAP_shift30'] = df['VWAP'].shift(30) / df[\"VWAP\"]\n            df['VWAP_shift45'] = df['VWAP'].shift(45) / df[\"VWAP\"]\n            df['VWAP_shift90'] = df['VWAP'].shift(90) / df[\"VWAP\"]\n            #df['VWAP_shift120'] = df['VWAP'].shift(120) / df[\"VWAP\"]\n            df['VWAP_shift180'] = df['VWAP'].shift(180) / df[\"VWAP\"]\n            df['VWAP_shift210'] = df['VWAP'].shift(210) / df[\"VWAP\"]\n            #df['VWAP_shift240'] = df['VWAP'].shift(240) / df[\"VWAP\"]\n            df['VWAP_shift310'] = df['VWAP'].shift(310) / df[\"VWAP\"]\n            df['VWAP_shift350'] = df['VWAP'].shift(350) / df[\"VWAP\"]\n            #df['VWAP_shift400'] = df['VWAP'].shift(400) / df[\"VWAP\"]\n            df['VWAP_shift450'] = df['VWAP'].shift(450) / df[\"VWAP\"]\n            df['VWAP_shift550'] = df['VWAP'].shift(550) / df[\"VWAP\"]\n            df['VWAP_shift600'] = df['VWAP'].shift(600) / df[\"VWAP\"]\n            df['VWAP_shift650'] = df['VWAP'].shift(650) / df[\"VWAP\"]\n            #df['VWAP_shift750'] = df['VWAP'].shift(750) / df[\"VWAP\"]\n            df['VWAP_shift800'] = df['VWAP'].shift(800) / df[\"VWAP\"]\n            df['VWAP_shift1000'] = df['VWAP'].shift(1000) / df[\"VWAP\"]\n            df['VWAP_shift1500'] = df['VWAP'].shift(1500) / df[\"VWAP\"]\n            df['VWAP_shift2000'] = df['VWAP'].shift(2000) / df[\"VWAP\"]\n            df['VWAP_shift2500'] = df['VWAP'].shift(2500) / df[\"VWAP\"]\n            df['VWAP_shift3000'] = df['VWAP'].shift(3000) / df[\"VWAP\"]\n            df['VWAP_shift3500'] = df['VWAP'].shift(3500) / df[\"VWAP\"]\n            df['VWAP_shift4000'] = df['VWAP'].shift(4000) / df[\"VWAP\"]\n            df['VWAP_shift4500'] = df['VWAP'].shift(4500) / df[\"VWAP\"]\n            df['VWAP_shift5000'] = df['VWAP'].shift(5000) / df[\"VWAP\"]\n            df['VWAP_shift5500'] = df['VWAP'].shift(5500) / df[\"VWAP\"]\n            df['VWAP_shift6000'] = df['VWAP'].shift(6000) / df[\"VWAP\"]\n            df['VWAP_shift7000'] = df['VWAP'].shift(7000) / df[\"VWAP\"]\n            df['VWAP_shift8000'] = df['VWAP'].shift(8000) / df[\"VWAP\"]\n            df['VWAP_shift9000'] = df['VWAP'].shift(9000) / df[\"VWAP\"]\n            df['VWAP_shift10000'] = df['VWAP'].shift(10000) / df[\"VWAP\"]\n            df['VWAP_shift11000'] = df['VWAP'].shift(11000) / df[\"VWAP\"]\n            df['VWAP_shift12000'] = df['VWAP'].shift(12000) / df[\"VWAP\"]\n            df['VWAP_shift14000'] = df['VWAP'].shift(14000) / df[\"VWAP\"]\n            df['VWAP_shift16000'] = df['VWAP'].shift(16000) / df[\"VWAP\"]\n            df['VWAP_shift18000'] = df['VWAP'].shift(18000) / df[\"VWAP\"]\n            df['VWAP_shift25000'] = df['VWAP'].shift(25000) / df[\"VWAP\"]\n#             df['Volume_shift_sma2000'] = (df['Volume'].rolling(2000).sum() / df[\"Volume\"]).apply(np.log)\n#             df['Close_shift70'] = df['Close'].shift(70) / df[\"Close\"]\n#             df['Close_shift280'] = df['Close'].shift(280) / df[\"Close\"]\n#             df['SMA_18000_std'] = df['Close'].rolling(18000).std().shift() / df['Close']\n            df['ROCP'] = talib.ROCP(df['Close'], timeperiod=181)\n            df['MOM'] = talib.MOM(df['Close'], timeperiod=176)\n#             df['MOM_1700'] = talib.MOM(df['Close'], timeperiod=1760)\n            df['RSI'] = talib.RSI(df['Close'], timeperiod=206)\n            #df['EMA'] = (talib.EMA(df['Close'], timeperiod=11) - df['Close']) / df[\"Close\"]\n#             df['APO'] = talib.APO(df['Close'], fastperiod=117, slowperiod=166, matype=0)\n#             df['CMO'] = talib.CMO(df['Close'], timeperiod=204)\n            macdhist = self.MACD(df['Close'], fastperiod=324, slowperiod=296, signalperiod=272)\n            df['macdhist0'] = macdhist\n            df['macdhist1'] = macdhist.shift(1)\n            df['macdhist5'] = macdhist.shift(5)\n            df['macdhist15'] = macdhist.shift(15)\n#             df['ADX'] = talib.ADX(df[\"High\"], df[\"Low\"], df[\"Close\"], timeperiod=181)\n            df['LINEARREG_ANGLE'] = talib.LINEARREG_ANGLE(df[\"Close\"], timeperiod=181)\n            df['HT_DCPERIOD'] = talib.HT_DCPERIOD(df[\"Close\"])\n#             df['NATR'] = talib.NATR(df[\"High\"], df[\"Low\"], df[\"Close\"], timeperiod=181)\n            df['AROONOSC'] = talib.AROONOSC(df[\"High\"], df[\"Low\"], timeperiod=181)\n            aroondown, aroonup = talib.AROON(df[\"High\"], df[\"Low\"], timeperiod=181)\n            df['aroondown'] = aroondown\n            df['aroonup'] = aroonup\n            df['PLUS_DI'] = talib.PLUS_DI(df[\"High\"], df[\"Low\"], df[\"Close\"], timeperiod=181)\n            upper1, middle,lower1 = talib.BBANDS(df[\"Close\"], timeperiod=181, nbdevup=3, nbdevdn=3, matype=0)\n            df['BBANDS'] = (upper1 - df['Close']).apply(np.log)\n#             df['CDL2CROWS'] = talib.CDL2CROWS(df[\"Open\"], df[\"High\"], df[\"Low\"], df[\"Close\"])\n\n#             categorical_features = ['CDL2CROWS']\n            \n            delete_columns = ['Asset_ID','High','Low','Close','Open','VWAP','Volume','Count']\n            #df.drop(delete_columns, axis=1, inplace=True)\n            for colum in delete_columns:\n                del df[colum]; gc.collect();\n                \n            df.reset_index(drop=True, inplace=True)\n            \n            ans = df.copy()\n        else:\n            \n            start = time.time()\n            ans = pd.DataFrame(index=[0], columns=[])\n            \n            fe_list = ['VWAP_shift15','VWAP_shift45','VWAP_shift90','VWAP_shift180','VWAP_shift210','VWAP_shift310','VWAP_shift350','VWAP_shift450','ROCP','MOM','RSI','macdhist0','macdhist1','macdhist5','macdhist15','LINEARREG_ANGLE','HT_DCPERIOD','AROONOSC','aroondown','aroonup','PLUS_DI','BBANDS']\n            future_list = []\n            with futures.ThreadPoolExecutor() as executor:\n                for i in range(len(fe_list)):\n                    # タスクを追加する。\n                    future = executor.submit(self.task, i, df)\n                    # Future オブジェクトを記録する。\n                    future_list.append(future)\n            print (\"0.0elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n            \n            start = time.time()\n            for i, x in enumerate(future_list):\n                ans[fe_list[i]] = x.result()\n            print (\"0.1elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n            \n\n            \n        start = time.time()\n        \n        ans.replace([np.inf, -np.inf], np.nan, inplace=True)\n        ans.fillna(method='ffill', inplace=True)\n        ans.fillna(0, inplace=True)\n        \n#         dfX = df.copy()\n#         dfX.drop(categorical_features, axis=1, inplace=True)\n#         columns = dfX.columns.tolist()\n#         X = dfX.values\n#         if not test:\n#             del dfX\n#             gc.collect()\n        \"\"\"正規化 sklearn\n        scaler = StandardScaler()\n        scaler.fit(X)\n        X = scaler.transform(X)\"\"\"\n        \n        print (\"1.3elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        start = time.time()\n        \n        #最小値の計算\n        #print(ans.columns.values.tolist()) \n#         mean_p = [-1.5995870828963125e-06, 0.9999936105140329, 0.9999798805796092, 0.999957909646952, 0.9999094811182778, 0.9998935809324517, 0.9998386623475682, 0.9998166999303114, 0.999761023451013, 2.68893869435963, 50.17742425201508, 0.36609391023010945, -8.513729049383712e-06, -8.536061613701882e-06, -8.62574773048785e-06, -8.817869229881006e-06, 17.394611336872575, 0.8841851233916151, 21.31082820733216, 0.7922342501475305, 2.1344215132277884, 44.66257903897539, 46.7970005522039, 7.528460106582875, 4.479986553198653, -0.05065731832118273]\n#         std_p = [0.0020139245973895704, 0.00526436090984338, 0.009028368384249637, 0.012659549928699778, 0.01764081834023001, 0.019017750981543444, 0.022929407656516868, 0.024322060457582173, 0.02745090276263502, 338.29863320746654, 3.292447808247903, 6.531061504548422, 0.08495146561479067, 0.08495145986995727, 0.08495143670699436, 0.0849513938680594, 15.733174955003946, 35.995526034504415, 5.35233708812034, 1.0490572699388196, 55.95956070093342, 33.11596376614276, 33.72480547239823, 4.476566042660567, 1.3812689854022944, 2.2501479214649804]\n# #         mean_p = pd.Series(mean_p, index=['Target', 'VWAP_shift15', 'VWAP_shift45', 'VWAP_shift90', 'VWAP_shift180', 'VWAP_shift210', 'VWAP_shift310', 'VWAP_shift350', 'VWAP_shift450', 'MOM', 'RSI', 'CMO', 'macdhist0', 'macdhist1', 'macdhist5', 'macdhist15', 'ADX', 'LINEARREG_ANGLE', 'HT_DCPERIOD', 'NATR', 'AROONOSC', 'aroondown', 'aroonup', 'PLUS_DI', 'BBANDS', 'CDL2CROWS'])\n# #         std_p = pd.Series(std_p, index=['Target', 'VWAP_shift15', 'VWAP_shift45', 'VWAP_shift90', 'VWAP_shift180', 'VWAP_shift210', 'VWAP_shift310', 'VWAP_shift350', 'VWAP_shift450', 'MOM', 'RSI', 'CMO', 'macdhist0', 'macdhist1', 'macdhist5', 'macdhist15', 'ADX', 'LINEARREG_ANGLE', 'HT_DCPERIOD', 'NATR', 'AROONOSC', 'aroondown', 'aroonup', 'PLUS_DI', 'BBANDS', 'CDL2CROWS'])\n#         mean_p = pd.Series(mean_p, index=ans.columns.values.tolist())\n#         std_p = pd.Series(std_p, index=ans.columns.values.tolist())\n#         #mean_p = ans.mean()\n#         #print(mean_p)\n#         #最大値の計算\n#         #std_p = ans.std(ddof = 0)\n#         #print(std_p.tolist())\n#         #標準化の計算\n#         ans = (ans - mean_p) / (std_p)\n\n        if save_fet:\n            ans.to_csv(save_name+'.csv', index=False)\n            \n        if save_mem:\n            ans = reduce_mem_usage(ans)\n        \n        if not test:\n            del df_list, df\n            gc.collect()\n        print (\"1.4elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n\n        return ans\n    \n    def normalize(self, df):\n        return (df - df.mean()) / df.std(ddof=0)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T06:26:38.725673Z","iopub.execute_input":"2022-01-23T06:26:38.725997Z","iopub.status.idle":"2022-01-23T06:26:38.797231Z","shell.execute_reply.started":"2022-01-23T06:26:38.725964Z","shell.execute_reply":"2022-01-23T06:26:38.796134Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"# Util","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nimport numpy as np\n\nclass Util():\n\n    def __init__(self) -> None:\n        pass\n\n    def data_conv(self, data):\n        #data = (data > 0.5).astype(int)\n        return data\n\n\n    def accuracy_score(self, train, predict):\n        \"\"\"\n        どのくらい答えに近いか評価するスコアを出す\n        あっているほど数値が高いようにする\n        コンペによって評価方法が違うからこれを変える\n        \"\"\"\n        #ピアソンの相関係数\n        return np.corrcoef(train,predict)[0,1]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:44:48.436124Z","iopub.execute_input":"2022-01-23T05:44:48.436436Z","iopub.status.idle":"2022-01-23T05:44:48.478530Z","shell.execute_reply.started":"2022-01-23T05:44:48.436391Z","shell.execute_reply":"2022-01-23T05:44:48.477539Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# モデル","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import ElasticNet\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import Pool\nfrom catboost import CatBoost, CatBoostRegressor, CatBoostClassifier\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport pandas as pd\nimport pickle\n\nfrom memory_profiler import profile\nimport gc\n\n\nut = Util()\n\nclass Models:\n    def __init__(self,ID) -> None:\n        fileID = ID\n        self.models_xgboost = []\n        for fold_id in range(4):\n            try:\n                with open('xgboost'+str(fileID)+'0'+str(fold_id)+'.pickle', 'rb') as web:\n                    self.models_xgboost.append(pickle.load(web))\n                #print('load')\n            except FileNotFoundError:\n                #print('Not find '+str(fileID)+'0'+str(fold_id))\n                pass\n    \n    def __del__(self):\n        \"\"\"オブジェクトが破棄されるとき呼び出される\"\"\"\n        print('Models died:', id(self))\n\n    def select_model(self, categorical_features, model_name, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None):\n        if model_name == \"random_forest\":\n            score, y_val_pre, y_pred = self.random_forest(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"light_gbm\":\n            score, y_val_pre, y_pred = self.light_gbm(categorical_features, learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == 'xgboost':\n            score, y_val_pre, y_pred = self.xgboost(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"catboost\":\n            score, y_val_pre, y_pred = self.catboost(categorical_features, learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"logistic_regression\":\n            score, y_val_pre, y_pred = self.logistic_regression(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        elif model_name == \"dnn\":\n            score, y_val_pre, y_pred = self.dnn(learn_type, fileID=fileID, X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, X_test=X_test)\n        else:\n            raise NameError(\"指定されたアルゴリズムは存在しません\")\n            \n\n        return score, y_val_pre, y_pred\n    \n    def KFold(self, categorical_features, model_name, learn_type, fileID=0, X_train=None, y_train=None, X_test=None, n_splits=4):\n\n        cv_score, oof_pre, y_sub = None, None, None\n        scores = []\n        oof_pre = np.array([])\n        valid_indexs = np.array([])\n        y_preds = []\n        cv = KFold(n_splits=n_splits, shuffle=True, random_state=0)\n        if learn_type=='learn':\n            for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, y_train)):\n                X_tr = X_train.loc[train_index, :]\n                X_val = X_train.loc[valid_index, :]\n                y_tr = y_train[train_index]\n                y_val = y_train[valid_index]\n\n                score, y_val_pre, _ = self.select_model(categorical_features, model_name, 'learn', fileID=str(fileID)+str(fold_id),X_train=X_tr, y_train=y_tr, X_valid=X_val, y_valid=y_val)\n\n                scores.append(score)\n                oof_pre = np.append(oof_pre,y_val_pre)\n                valid_indexs = np.append(valid_indexs, valid_index)\n\n            oof_pre = oof_pre[np.argsort(valid_indexs)]\n            cv_score = sum(scores) / len(scores)\n        elif learn_type=='predict':\n            future_list = []\n            with futures.ThreadPoolExecutor() as executor:\n                for fold_id in range(n_splits):\n                    future = executor.submit(self.select_model, categorical_features, model_name, 'predict', fileID=str(fileID)+str(fold_id), X_test=X_test)\n                    future_list.append(future)\n                    #score, y_val_pre, y_pred = self.select_model(categorical_features, model_name, 'predict', fileID=str(fileID)+str(fold_id), X_test=X_test)\n#                     oof_pre = np.append(oof_pre,y_val_pre)\n#                     y_preds.append(y_pred)\n                    \n            for i, x in enumerate(future_list):\n                score, y_val_pre, y_pred = x.result()\n                oof_pre = np.append(oof_pre,y_val_pre)\n                y_preds.append(y_pred)\n                    \n            \n            y_sub = sum(y_preds) / len(y_preds)\n\n        return cv_score, oof_pre, y_sub\n\n    def random_forest(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, n_estimators=67, max_depth=6, random_state=0):\n        \"\"\"\n        pandasでの教師データ\n        パラメータ\n        return valスコア(float)、その取り出し方での予測値\n        \"\"\"\n        print('========random_forest========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('RandomForest'+str(fileID)+'.pickle', 'rb') as web:\n                RandomForest = pickle.load(web)\n            y_pred = RandomForest.predict(X_test)\n        elif learn_type=='learn':\n            #RandomForest = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n            RandomForest = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, random_state=random_state)\n            RandomForest.fit(X_train, y_train)\n            y_val_pre = RandomForest.predict(X_valid)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('RandomForest'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(RandomForest , web)\n\n        return score, y_val_pre, y_pred\n\n    def light_gbm(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, params = {'objective': 'binary','max_bin': 284,'learning_rate': 0.068,'num_leaves': 45}):\n        \"\"\"\n        pandasでの教師データ\n        categorical_features:カテゴリかる属性のカラム名を示したリスト\n        パラメータ\n        return valスコア(float), y_val_pre(valでの予測値), その取り出し方での予測値\n        \"\"\"\n        print('========light_gbm========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('light_gbm'+str(fileID)+'.pickle', 'rb') as web:\n                model = pickle.load(web)\n            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            lgb_train = lgb.Dataset(X_train, y_train, categorical_feature=categorical_features)\n            lgb_eval = lgb.Dataset(X_valid, y_valid, reference=lgb_train, categorical_feature=categorical_features)\n            model = lgb.train(params, lgb_train,valid_sets=[lgb_train, lgb_eval],verbose_eval=10,num_boost_round=1000,early_stopping_rounds=10)\n\n            y_val_pre = model.predict(X_valid, num_iteration=model.best_iteration)\n            y_val_pre = ut.data_conv(y_val_pre)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('light_gbm'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(model , web)\n\n        return score, y_val_pre, y_pred\n    \n    def xgboost(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, params = {'tree_method': 'gpu_hist', 'objective': 'reg:squarederror','silent':1, 'random_state':0,'learning_rate': 0.15, 'eval_metric': 'rmse',}, num_round = 450):\n        \"\"\"\n        pandasでの教師データ\n        categorical_features:カテゴリかる属性のカラム名を示したリスト\n        パラメータ\n        return valスコア(float), y_val_pre(valでの予測値), その取り出し方での予測値\n        \"\"\"\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            test = xgb.DMatrix(X_test)\n            model = self.models_xgboost[int(str(fileID)[-1])]\n            y_pred = model.predict(test)\n        elif learn_type=='learn':\n            print('========xgboost========')\n            train = xgb.DMatrix(X_train, label=y_train)\n            valid = xgb.DMatrix(X_valid, label=y_valid)\n            self.model_xgboost = xgb.train(params,\n                    train,#訓練データ\n                    num_round,#設定した学習回数\n                    early_stopping_rounds=20,\n                    evals=[(train, 'train'), (valid, 'eval')],\n                    verbose_eval=100\n                    )\n            y_val_pre = self.model_xgboost.predict(valid)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('xgboost'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(self.model_xgboost, web)\n                \n        \n        \n#         _, ax = plt.subplots(figsize=(12, 15))\n#         xgb.plot_importance(self.model_xgboost,\n#                     ax=ax,\n#                     importance_type='gain',\n#                     show_values=False)\n#         plt.show()\n        \n        \n        return score, y_val_pre, y_pred\n\n    def catboost(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None, params ={'depth' : 3,'learning_rate' : 0.054,'early_stopping_rounds' : 9,'iterations' : 474, 'loss_function' : 'RMSE', 'random_seed' :0}):\n        \"\"\"\n        pandasでの教師データ\n        categorical_features:カテゴリかる属性のカラム名を示したリスト\n        パラメータ\n        return valスコア(float), y_val_pre(valでの予測値), その取り出し方での予測値\n        \"\"\"\n        print('========catboost========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('catboost'+str(fileID)+'.pickle', 'rb') as web:\n                model = pickle.load(web)\n            y_pred = model.predict(X_test)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            train = Pool(X_train, y_train, cat_features=categorical_features)\n            eval = Pool(X_valid, y_valid, cat_features=categorical_features)\n            #cab = CatBoostClassifier(custom_loss=['Accuracy'],random_seed=0)\n            #cab = CatBoostClassifier(**params)\n            cab = CatBoostRegressor(random_seed=0)\n            cab = CatBoostRegressor(**params)\n            model = cab.fit(train, eval_set=eval)\n\n            y_val_pre = model.predict(X_valid)\n            y_val_pre = ut.data_conv(y_val_pre)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('catboost'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(model , web)\n\n        return score, y_val_pre, y_pred\n\n    def logistic_regression(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None):\n        \"\"\"\n        pandasでの教師データ\n        パラメータ\n        return valスコア(float)、その取り出し方での予測値\n        \"\"\"\n        print('========logistic_regression========')\n        score, y_val_pre, y_pred = None, None, None\n        if learn_type=='predict':\n            with open('logistic_regression'+str(fileID)+'.pickle', 'rb') as web:\n                model = pickle.load(web)\n            y_pred = model.predict(X_test)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            #model = LogisticRegression(penalty='l2', solver='sag', random_state=0)\n            model = ElasticNet(random_state=0)\n            model.fit(X_train, y_train)\n            y_val_pre = model.predict(X_valid)\n            y_val_pre = ut.data_conv(y_val_pre)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            with open('logistic_regression'+str(fileID)+'.pickle', 'wb') as web:\n                pickle.dump(model , web)\n\n        return score, y_val_pre, y_pred\n\n    def dnn(self, learn_type, fileID=0, X_train=None, y_train=None, X_valid=None, y_valid=None, X_test=None):\n        \n        print('========dnn========')\n        score, y_val_pre, y_pred = None, None, None\n\n        lr_schedule=tf.keras.optimizers.schedules.ExponentialDecay( \\\n                    initial_learning_rate=0.001, #初期の学習率\n                    decay_steps=3, #減衰ステップ数\n                    decay_rate=0.01, #最終的な減衰率 \n                    staircase=True)\n\n        model=Sequential()\n        model.add(Dense(len(X_train.columns),input_shape=(len(X_train.columns),),activation='relu',\n                    kernel_regularizer=keras.regularizers.l2(0.001), #重みの正則化考慮\n                    kernel_initializer='random_uniform',\n                    bias_initializer='zero'))\n                    \n        model.add(BatchNormalization()) #バッチ正規化\n        model.add(Dropout(0.1)) # ドロップアウト層・ドロップアウトさせる割合\n        model.add(Dense(int(len(pd.DataFrame(X_train).columns)/2),activation='sigmoid'))\n\n        model.add(BatchNormalization()) #バッチ正規化\n        model.add(Dropout(0.1)) # ドロップアウト層・ドロップアウトさせる割合\n        model.add(Dense(int(len(pd.DataFrame(X_train).columns)/2),activation='sigmoid'))\n\n        model.add(BatchNormalization()) #バッチ正規化\n        model.add(Dropout(0.1)) # ドロップアウト層・ドロップアウトさせる割合\n        model.add(Dense(len(pd.DataFrame(y_train).columns),activation='sigmoid'))\n        Ecall=EarlyStopping(monitor='val_loss',patience=1000,restore_best_weights=False)\n        model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=lr_schedule))\n        model.summary()\n\n        if learn_type=='predict':\n            model.load_weights('dnn'+str(fileID)+'.h5')\n            y_pred = model.predict(X_test)\n            y_pred = ut.data_conv(y_pred)\n        elif learn_type=='learn':\n            res=model.fit(X_train.values,y_train.values,epochs=3,callbacks=[Ecall],verbose=1,validation_data=(X_valid.values,y_valid.values))\n            y_val_pre = model.predict(X_valid)[:,0]\n            y_val_pre = ut.data_conv(y_val_pre)\n            print(y_valid.shape)\n            print(y_val_pre.shape)\n            score = ut.accuracy_score(y_valid, y_val_pre)\n            model.save_weights('dnn'+str(fileID)+'.h5')\n\n        return score, y_val_pre, y_pred","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:47:16.198097Z","iopub.execute_input":"2022-01-23T05:47:16.198439Z","iopub.status.idle":"2022-01-23T05:47:16.275938Z","shell.execute_reply.started":"2022-01-23T05:47:16.198407Z","shell.execute_reply":"2022-01-23T05:47:16.274698Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# パラメータオプティマイザー","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport optuna\nfrom sklearn.metrics import log_loss\n\n#md = Models(0)\n\n#random_forest : {'n_estimators': 67, 'max_depth': 6}\n#light_gbm : {'max_bin': 284, 'learning_rate': 0.06759289191947715, 'num_leaves': 45}\n#xgboost : {'learning_rate': 0.180343853211702, 'num_round': 394}\n#catboost : {'depth': 3, 'learning_rate': 0.053925065258405916, 'early_stopping_rounds': 9, 'iterations': 474}\nclass Optimizer():\n    def __init__(self) -> None:\n        pass\n\n    def param_opt(self, model_name, X_train, y_train, categorical_features=None):\n        \"\"\"\n        パラメータオプティマイザー\n        model name list: random_forest, light_gbm\n        \"\"\"\n        X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.3,random_state=0, stratify=y_train)\n        study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n        if model_name == \"random_forest\":\n            study.optimize(self.objective_random_forest(X_train, y_train, X_valid, y_valid), n_trials=80)\n            return study.best_params\n\n        elif model_name == \"light_gbm\":\n            study.optimize(self.objective_light_gbm(X_train, y_train, X_valid, y_valid, categorical_features), n_trials=80)\n            return study.best_params\n\n        elif model_name == \"xgboost\":\n            study.optimize(self.objective_xgboost(X_train, y_train, X_valid, y_valid), n_trials=80)\n            return study.best_params\n\n        elif model_name == \"catboost\":\n            study.optimize(self.objective_catboost(X_train, y_train, X_valid, y_valid, categorical_features), n_trials=80)\n            return study.best_params\n            \n        elif model_name == 'logistic_regression':\n            raise NameError('logistic_regressionはパラメータが存在しないのでサポートしていません')\n\n    def objective_random_forest(self, X_train, y_train, X_valid, y_valid):\n        def objective(trial):\n            n_estimators = trial.suggest_int('n_estimators', 10, 300)\n            max_depth = trial.suggest_int('max_depth', 1, 15)\n            _, y_val_pre, _ = md.random_forest('learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, n_estimators=n_estimators, max_depth=max_depth, random_state=0)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    def objective_light_gbm(self, X_train, y_train, X_valid, y_valid, categorical_features):\n        def objective(trial):\n            params = {\n            'objective': 'binary',\n            'max_bin': trial.suggest_int('max_bin', 255, 500),\n            'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.1),\n            'num_leaves': trial.suggest_int('num_leaves', 32, 128),\n            }\n            _, y_val_pre, _ = md.light_gbm(categorical_features, 'learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, params=params)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    def objective_xgboost(self, X_train, y_train, X_valid, y_valid):\n        def objective(trial):\n            params = {'objective': 'reg:squarederror',\n                    'silent':1, \n                    'random_state':0,\n                    'learning_rate': trial.suggest_uniform('learning_rate', 0.01, 0.2), \n                    'eval_metric': 'rmse',\n            }\n            num_round = trial.suggest_int('num_round', 100, 900)\n            _, y_val_pre, _ = md.xgboost('learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, params=params, num_round=num_round)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    def objective_catboost(self, X_train, y_train, X_valid, y_valid, categorical_features):\n        def objective(trial):\n            params = {\n                'depth' : trial.suggest_int('depth', 1, 15),                  # 木の深さ\n                'learning_rate' : trial.suggest_uniform('learning_rate', 0.01, 0.1),       # 学習率\n                'early_stopping_rounds' : trial.suggest_int('early_stopping_rounds', 3, 20),\n                'iterations' : trial.suggest_int('iterations', 50, 500), \n                'custom_loss' :['Accuracy'], \n                'random_seed' :0\n            }\n            _, y_val_pre, _ = md.catboost(categorical_features, 'learn', X_train=X_train, y_train=y_train, X_valid=X_valid, y_valid=y_valid, params=params)\n            \n            score = log_loss(y_valid, y_val_pre)\n\n            return score\n        return objective\n\n    #LogisticRegressionはパラメータがない","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:47:23.060755Z","iopub.execute_input":"2022-01-23T05:47:23.061142Z","iopub.status.idle":"2022-01-23T05:47:23.085096Z","shell.execute_reply.started":"2022-01-23T05:47:23.061107Z","shell.execute_reply":"2022-01-23T05:47:23.084010Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# アンサンブル","metadata":{}},{"cell_type":"code","source":"from sklearn import ensemble\nimport numpy as np\nimport pandas as pd\n\n#md = Models()\nut = Util()\n\nclass Ensemble():\n\n    def __init__(self) -> None:\n        pass\n\n    def stacking(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_test=None, fst_lay=['random_forest', 'light_gbm', 'xgboost', 'catboost'], snd_lay='light_gbm', enable_2ndorigx=True):\n        #enable_2ndorigx:二層目にオリジナルの入力データを入力するか\n\n        stack_oof_pred = []\n        stack_pred = []\n        for index, model_name in enumerate(fst_lay):\n            \n            if learn_type=='learn':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_train=X_train, y_train=y_train)\n                stack_oof_pred = oof_pre if index == 0 else np.c_[stack_oof_pred, oof_pre]\n            elif learn_type=='predict':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_test=X_test)\n                stack_pred = y_sub if index == 0 else np.c_[stack_pred, y_sub]\n            else:\n                raise NameError(\"指定されたlearn_typeは存在しません\")\n\n        if enable_2ndorigx:\n            X_train2 =  pd.concat([pd.DataFrame(stack_oof_pred), X_train], axis=1)\n            X_test2 =  pd.concat([pd.DataFrame(stack_pred), X_test], axis=1)\n        else:\n            X_train2 = pd.DataFrame(stack_oof_pred)\n            X_test2 = pd.DataFrame(stack_pred)\n            categorical_features = []\n\n        #二層目\n        if learn_type=='learn':\n            cv_score, oof_pre, y_sub = md.KFold(categorical_features, snd_lay, learn_type, fileID=fileID+'1', X_train=X_train2, y_train=y_train)\n        elif learn_type=='predict':\n            cv_score, oof_pre, y_sub = md.KFold(categorical_features, snd_lay, learn_type, fileID=fileID+'1', X_test=X_test2)\n            y_sub = ut.data_conv(y_sub)\n\n        return cv_score, oof_pre, y_sub\n\n    def mean(self, categorical_features, learn_type, fileID=0, X_train=None, y_train=None, X_test=None, models=['random_forest', 'light_gbm', 'xgboost', 'catboost'], type='mean'):\n        '''\n        type:平均の取り方 \n        mean -> 算術平均\n        hmean -> 調和平均\n        gmean -> 幾何平均\n        '''\n\n        stack_oof_pred = []\n        stack_pred = []\n        for index, model_name in enumerate(models):\n            if learn_type=='learn':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_train=X_train, y_train=y_train)\n                stack_oof_pred = oof_pre if index == 0 else np.c_[stack_oof_pred, oof_pre]\n            elif learn_type=='predict':\n                cv_score, oof_pre, y_sub = md.KFold(categorical_features, model_name, learn_type, fileID=fileID+'0', X_test=X_test)\n                stack_pred = y_sub if index == 0 else np.c_[stack_pred, y_sub]\n            else:\n                raise NameError(\"指定されたlearn_typeは存在しません\")\n\n        if type == 'mean':\n            y_off = np.average(stack_oof_pred, axis=1)\n            y_sub = np.average(stack_pred, axis=1)\n        elif type == 'hmean':\n            from scipy.stats import hmean\n            y_off = hmean(stack_oof_pred, axis = 1)\n            y_sub = hmean(stack_pred, axis = 1)\n        elif type == 'gmean':\n            from scipy.stats.mstats import gmean\n            y_off = gmean(stack_oof_pred, axis = 1)\n            y_sub = gmean(stack_pred, axis = 1)\n        \n        y_off = ut.data_conv(y_off)\n        y_sub = ut.data_conv(y_sub)\n\n        cv_score = ut.accuracy_score(y_train, y_off)\n        \n        return cv_score, oof_pre, y_sub","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:47:28.753626Z","iopub.execute_input":"2022-01-23T05:47:28.753949Z","iopub.status.idle":"2022-01-23T05:47:28.777166Z","shell.execute_reply.started":"2022-01-23T05:47:28.753906Z","shell.execute_reply":"2022-01-23T05:47:28.775835Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# コントローラー","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nop = Optimizer()\nens = Ensemble()\n\nclass Controller():\n\n    def __init__(self,ID) -> None:\n        self.md = Models(ID)\n        self.ID = str(ID)\n\n    def opt(self, X_train, y_train):\n        print(op.param_opt('light_gbm', X_train, y_train))\n\n    def KFold_learn(self, categorical_features, X_train, y_train, model_name):\n        cv_score, y_val_pre, y_sub = self.md.KFold(categorical_features, model_name, 'learn', fileID=self.ID+'0', X_train=X_train, y_train=y_train)\n\n        print('CV score-----------------------------------',cv_score)\n        #random_forest 0.822635113928818\n        #light_gbm 0.8293829640323895\n        #catboost 0.8204004770573097\n        #logistic_regression 0.6846023476241291\n        #xgboost 0.8192643274119641\n        #dnn 0.8159060950348378s\n        \n        return cv_score\n\n    def KFold_predict(self, categorical_features, X_test, model_name):\n        cv_score, y_val_pre, y_sub = self.md.KFold(categorical_features, model_name, 'predict', fileID=self.ID+'0', X_test=X_test)\n        \n        return y_sub\n\n    def stacking_learn(self, categorical_features, X_train, y_train, fst_lay=['random_forest', 'light_gbm', 'xgboost', 'catboost'], snd_lay='light_gbm', enable_2ndorigx=False):\n        cv_score, _, y_sub = ens.stacking(categorical_features, 'learn', fileID=self.ID, X_train=X_train, y_train=y_train, fst_lay=fst_lay, snd_lay=snd_lay, enable_2ndorigx=enable_2ndorigx)\n\n        print('CV score-----------------------------------',cv_score)\n        #0.8293829640323895 2ndlgtm\n        #0.8237712635741635 2ndrandomforest\n\n    def stacking_predict(self, categorical_features, X_test, fst_lay=['random_forest', 'light_gbm', 'xgboost', 'catboost'], snd_lay='light_gbm', enable_2ndorigx=False):\n        _, _, y_sub = ens.stacking(categorical_features, 'predict', fileID=self.ID, X_test=X_test, fst_lay=fst_lay, snd_lay=snd_lay, enable_2ndorigx=enable_2ndorigx)\n\n        sub = pd.read_csv('input/titanic/gender_submission.csv')\n        sub['Survived'] = y_sub\n        sub.to_csv('submission.csv', index=False)\n        #0.8293829640323895\n\n    def mean_learn(self, categorical_features, learn_type, X_train, y_train, models=['random_forest', 'light_gbm', 'xgboost', 'catboost'], type='mean'):\n        cv_score, _, y_sub = ens.mean(categorical_features, learn_type, fileID=self.ID, X_train=X_train, y_train=y_train, models=models, type=type)\n        #mean:0.8237934904601572 hmean:0.819304152637486 gmean:0.819304152637486\n        print('CV score-----------------------------------',cv_score)\n\n    def mean_predict(self, categorical_features, learn_type, X_test, models=['random_forest', 'light_gbm', 'xgboost', 'catboost'], type='mean'):\n        cv_score, _, y_sub = ens.mean(categorical_features, learn_type, fileID=self.ID, X_test=X_test, models=models, type=type)\n\n        sub = pd.read_csv('input/titanic/gender_submission.csv')\n        sub['Survived'] = y_sub\n        sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:47:31.877330Z","iopub.execute_input":"2022-01-23T05:47:31.878039Z","iopub.status.idle":"2022-01-23T05:47:31.896219Z","shell.execute_reply.started":"2022-01-23T05:47:31.878001Z","shell.execute_reply":"2022-01-23T05:47:31.895179Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"markdown","source":"教師データ読み込み","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport random\nimport os\n\ndef seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\nseed_everything\n\ndata_folder = \"../input/g-research-crypto-forecasting/\"\n#crypto_df = reduce_mem_usage(pd.read_csv(data_folder + 'train.csv'))\ncrypto_df = pd.read_csv(data_folder + 'train.csv')\n\ntrain_list = list(range(13))\nfor Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n    #train = reduce_mem_usage(crypto_df[crypto_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\"))\n    train = crypto_df[crypto_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n    train_list[Asset_ID] = train\ndel crypto_df\ndel train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:44:57.462355Z","iopub.execute_input":"2022-01-23T05:44:57.463206Z","iopub.status.idle":"2022-01-23T05:45:57.902634Z","shell.execute_reply.started":"2022-01-23T05:44:57.463161Z","shell.execute_reply":"2022-01-23T05:45:57.901653Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"22"},"metadata":{}}]},{"cell_type":"markdown","source":"## 学習","metadata":{}},{"cell_type":"code","source":"scores = []\nfe = Feature()\nfor Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n    \n    print('通貨番号',Asset_ID)\n    \n    train_raw = train_list[Asset_ID].copy()\n    y_train = train_raw['Target'].copy()\n    X_train = train_raw.drop('Target', axis=1).copy()\n    \n    y_train.reset_index(drop=True, inplace=True)\n    y_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n    y_train.fillna(method='ffill', inplace=True)\n    y_train.fillna(0, inplace=True)\n    X_train = fe.conv_data(X_train, Asset_ID, train_list, save_fet=False, save_name='feature')\n    #print(train)\n    #print(train_list[1])\n    categorical_features = ['CDL2CROWS']\n    \n    ct = Controller(Asset_ID)\n            \n    cv_score = ct.KFold_learn(categorical_features, X_train, y_train, 'xgboost')\n    scores.append(cv_score)\n    #ct.stacking_learn(categorical_features, X_train, y_train, fst_lay=['dnn', 'light_gbm', 'xgboost'], snd_lay='light_gbm', enable_2ndorigx=False)\n    \n    del train_raw, y_train, X_train, ct\n    gc.collect()\n    \n    print('5',psutil.virtual_memory().percent)\n    \n    \nprint('CV mean-----------------------------'+str(sum(scores)/len(scores)))\n\n#CV score----------------------------------- 0.23738583123946141 light_gbm 0.2578\n#xgboost 0.44045052797107087\n#random forest 微妙　0.15 \n#catboost 0.06","metadata":{"execution":{"iopub.status.busy":"2022-01-23T06:26:47.866980Z","iopub.execute_input":"2022-01-23T06:26:47.867295Z","iopub.status.idle":"2022-01-23T06:28:00.242492Z","shell.execute_reply.started":"2022-01-23T06:26:47.867262Z","shell.execute_reply":"2022-01-23T06:28:00.240905Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"通貨番号 0\nFeature died: 140114302901392\n1.3elapsed_time:0.9051985740661621[sec]\nMem. usage decreased to 185.26 Mb (75.0% reduction)\n1.4elapsed_time:8.327182054519653[sec]\n========xgboost========\n[06:27:08] WARNING: ../src/learner.cc:576: \nParameters: { \"silent\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\ttrain-rmse:0.42501\teval-rmse:0.42501\n[100]\ttrain-rmse:0.00470\teval-rmse:0.00482\n[200]\ttrain-rmse:0.00443\teval-rmse:0.00459\n[300]\ttrain-rmse:0.00424\teval-rmse:0.00444\n[400]\ttrain-rmse:0.00408\teval-rmse:0.00431\n[449]\ttrain-rmse:0.00402\teval-rmse:0.00426\n========xgboost========\n[06:27:19] WARNING: ../src/learner.cc:576: \nParameters: { \"silent\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\ttrain-rmse:0.42501\teval-rmse:0.42501\n[100]\ttrain-rmse:0.00470\teval-rmse:0.00483\n[200]\ttrain-rmse:0.00444\teval-rmse:0.00461\n[300]\ttrain-rmse:0.00425\teval-rmse:0.00445\n[400]\ttrain-rmse:0.00409\teval-rmse:0.00433\n[449]\ttrain-rmse:0.00403\teval-rmse:0.00427\n========xgboost========\n[06:27:29] WARNING: ../src/learner.cc:576: \nParameters: { \"silent\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\ttrain-rmse:0.42501\teval-rmse:0.42502\n[100]\ttrain-rmse:0.00471\teval-rmse:0.00484\n[200]\ttrain-rmse:0.00444\teval-rmse:0.00463\n[300]\ttrain-rmse:0.00425\teval-rmse:0.00448\n[400]\ttrain-rmse:0.00410\teval-rmse:0.00436\n[449]\ttrain-rmse:0.00403\teval-rmse:0.00431\n========xgboost========\n[06:27:39] WARNING: ../src/learner.cc:576: \nParameters: { \"silent\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\n[0]\ttrain-rmse:0.42501\teval-rmse:0.42500\n[100]\ttrain-rmse:0.00471\teval-rmse:0.00484\n[200]\ttrain-rmse:0.00444\teval-rmse:0.00462\n[300]\ttrain-rmse:0.00425\teval-rmse:0.00445\n[400]\ttrain-rmse:0.00409\teval-rmse:0.00433\n[449]\ttrain-rmse:0.00402\teval-rmse:0.00427\nCV score----------------------------------- 0.6372799232410368\nModels died: 140118156350160\n5 31.5\n通貨番号 1\n1.3elapsed_time:0.7872345447540283[sec]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/3091970970.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ffill'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAsset_ID\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_fet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'feature'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;31m#print(train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#print(train_list[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_34/222480733.py\u001b[0m in \u001b[0;36mconv_data\u001b[0;34m(self, df, Asset_ID, df_list, save_fet, save_name, save_mem, test)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_mem\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_mem_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_34/4141208316.py\u001b[0m in \u001b[0;36mreduce_mem_usage\u001b[0;34m(df, verbose)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mc_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc_max\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mc_min\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mc_max\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3611\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3795\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexisting_piece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3797\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3799\u001b[0m     def _set_value(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item_mgr\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3754\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3756\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iset_item_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3758\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_iset_item_mgr\u001b[0;34m(self, loc, value)\u001b[0m\n\u001b[1;32m   3744\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_iset_item_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mslice\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3745\u001b[0m         \u001b[0;31m# when called from _set_item_mgr loc can be anything returned from get_loc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3746\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3747\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36miset\u001b[0;34m(self, loc, value)\u001b[0m\n\u001b[1;32m   1085\u001b[0m                     \u001b[0mremoved_blknos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblkno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m                     \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1088\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4407\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0marr\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0melements\u001b[0m \u001b[0mspecified\u001b[0m \u001b[0mby\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mremoved\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mNote\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4408\u001b[0m         \u001b[0mthat\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mdoes\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moccur\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4409\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0mflattened\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4411\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0mAlso\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"#Measure a time(fe)\nfe = Feature()\ndf = train_list[1].copy()\nstart = time.time()\n#df = fe.conv_data(df, 1, train_list, save_fet=False, save_name='feature', save_mem=False)\ndf = fe.conv_data(df.iloc[-610:,:], 1, train_list, save_fet=False, save_name='feature', save_mem=False, test=True)\nprint (\"0elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\ndisplay(df.iloc[-1,:],'ans')\ndisplay(df.info,'ans')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T06:13:43.542354Z","iopub.execute_input":"2022-01-23T06:13:43.542980Z","iopub.status.idle":"2022-01-23T06:13:43.632170Z","shell.execute_reply.started":"2022-01-23T06:13:43.542944Z","shell.execute_reply":"2022-01-23T06:13:43.631141Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"Feature died: 140114321170768\n0.0elapsed_time:0.023078203201293945[sec]\n0.1elapsed_time:0.009051322937011719[sec]\n1.3elapsed_time:0.0012247562408447266[sec]\n1.4elapsed_time:1.1920928955078125e-06[sec]\n0elapsed_time:0.034983158111572266[sec]\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py:5182: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  downcast=downcast,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VWAP_shift15         0.992272\nVWAP_shift45         1.005135\nVWAP_shift90         1.017850\nVWAP_shift180        1.015129\nVWAP_shift210        1.015822\nVWAP_shift310        1.013320\nVWAP_shift350        1.021136\nVWAP_shift450        1.023971\nROCP                -0.012434\nMOM               -580.377500\nRSI                 46.443191\nmacdhist0            0.043689\nmacdhist1            0.044011\nmacdhist5            0.043150\nmacdhist15           0.033535\nLINEARREG_ANGLE    -80.578859\nHT_DCPERIOD         18.429952\nAROONOSC           -30.386740\naroondown           84.530387\naroonup             54.143646\nPLUS_DI             17.100123\nBBANDS               7.386793\nName: 0, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'ans'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<bound method DataFrame.info of    VWAP_shift15  VWAP_shift45  VWAP_shift90  VWAP_shift180  VWAP_shift210  \\\n0      0.992272      1.005135       1.01785       1.015129       1.015822   \n\n   VWAP_shift310  VWAP_shift350  VWAP_shift450      ROCP       MOM  ...  \\\n0        1.01332       1.021136       1.023971 -0.012434 -580.3775  ...   \n\n   macdhist1  macdhist5  macdhist15  LINEARREG_ANGLE  HT_DCPERIOD  AROONOSC  \\\n0   0.044011    0.04315    0.033535       -80.578859    18.429952 -30.38674   \n\n   aroondown    aroonup    PLUS_DI    BBANDS  \n0  84.530387  54.143646  17.100123  7.386793  \n\n[1 rows x 22 columns]>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'ans'"},"metadata":{}}]},{"cell_type":"code","source":"# #del train_raw\n# del X_train, y_train\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.800096Z","iopub.status.idle":"2022-01-23T05:46:45.801022Z","shell.execute_reply.started":"2022-01-23T05:46:45.800667Z","shell.execute_reply":"2022-01-23T05:46:45.800701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# del train_list\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.802373Z","iopub.status.idle":"2022-01-23T05:46:45.803540Z","shell.execute_reply.started":"2022-01-23T05:46:45.803165Z","shell.execute_reply":"2022-01-23T05:46:45.803230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import sys\n\n# print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n# print(\" ------------------------------------ \")\n# for var_name in dir():\n#     if not var_name.startswith(\"_\"):\n#         print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.805214Z","iopub.status.idle":"2022-01-23T05:46:45.808293Z","shell.execute_reply.started":"2022-01-23T05:46:45.807923Z","shell.execute_reply":"2022-01-23T05:46:45.807988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#train_list[0].reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.809772Z","iopub.status.idle":"2022-01-23T05:46:45.811061Z","shell.execute_reply.started":"2022-01-23T05:46:45.810735Z","shell.execute_reply":"2022-01-23T05:46:45.810765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#fe.conv_data(train_list[0], save_fet=False, save_name='feature')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.812637Z","iopub.status.idle":"2022-01-23T05:46:45.813941Z","shell.execute_reply.started":"2022-01-23T05:46:45.813595Z","shell.execute_reply":"2022-01-23T05:46:45.813629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## predict","metadata":{}},{"cell_type":"code","source":"supplemental_df = pd.read_csv(data_folder + 'supplemental_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.815514Z","iopub.status.idle":"2022-01-23T05:46:45.816175Z","shell.execute_reply.started":"2022-01-23T05:46:45.815860Z","shell.execute_reply":"2022-01-23T05:46:45.815907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_list = list(range(13))\nfor Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n    supplemental_train = supplemental_df[supplemental_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n    if len(supplemental_train) > 1000:#30000\n        #supplemental_df.drop(supplemental_df.index[-26000:], inplace=True)\n        supplemental_train = supplemental_train.iloc[-1000:,:]\n    test_list[Asset_ID] = supplemental_train","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.818426Z","iopub.status.idle":"2022-01-23T05:46:45.819026Z","shell.execute_reply.started":"2022-01-23T05:46:45.818670Z","shell.execute_reply":"2022-01-23T05:46:45.818702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gresearch_crypto\n#以下二つは1セッションで一度しか実行できない\nenv = gresearch_crypto.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.821002Z","iopub.status.idle":"2022-01-23T05:46:45.821879Z","shell.execute_reply.started":"2022-01-23T05:46:45.821512Z","shell.execute_reply":"2022-01-23T05:46:45.821551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfe = Feature()\nctlist = []\ncategorical_features = ['CDL2CROWS']\nfor Asset_ID in range(13):\n    ct = Controller(Asset_ID)\n    ctlist.append(ct)\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    start1 = time.time()\n    start = time.time()\n    for Asset_ID in range(13):\n        test_raw = df_test[df_test[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n        test = pd.concat([test_list[Asset_ID], test_raw], sort=False)\n        if len(test) > 1000:#30000\n            #test.drop(test.index[-26000:], inplace=True)\n            test = test.iloc[-1000:,:]\n        test_list[Asset_ID] = test.copy()\n    print (\"1elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n    for Asset_ID in range(13):\n        start = time.time()\n        print('通貨番号',Asset_ID)\n\n        test_raw = test_list[Asset_ID].copy()\n        \n        #print(test_raw)\n        \n        row_id = test_raw.iat[-1, 9]\n        print('row_id :',row_id)\n        X_test = test_raw.drop(['Target','row_id'], axis=1).copy()\n        print (\"2elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        start = time.time()\n        X_test = fe.conv_data(X_test, Asset_ID, test_list, save_fet=False, save_name='feature', save_mem=False, test=True)\n        print (\"3elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        start = time.time()\n        ct = ctlist[Asset_ID]\n        y_sub = ct.KFold_predict(categorical_features, X_test, 'xgboost')[-1]\n        #y_sub = ct.stacking_predict(categorical_features, X_test)\n        print (\"4elapsed_time:{0}\".format(time.time() - start) + \"[sec]\")\n        \n        print('pred :',y_sub)\n        #print(df_pred)\n        #print(df_pred['row_id'] == row_id)\n        \n        df_pred.loc[df_pred['row_id'] == row_id, 'Target'] = y_sub\n    \n    #print(df_pred)\n    print (\"elapsed_time:{0}\".format(time.time() - start1) + \"[sec]\")\n    env.predict(df_pred)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.823347Z","iopub.status.idle":"2022-01-23T05:46:45.824062Z","shell.execute_reply.started":"2022-01-23T05:46:45.823719Z","shell.execute_reply":"2022-01-23T05:46:45.823753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fe = Feature()\nX_test = test_raw.drop('Target', axis=1).copy()\n\nprint(1)\nX_test = fe.conv_data(X_test, Asset_ID, test_list, save_fet=False, save_name='feature', save_mem=False).iloc[-1:]\nprint(2)\ny_sub = ct.KFold_predict(categorical_features, X_test, 'xgboost')\nprint(2)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.825940Z","iopub.status.idle":"2022-01-23T05:46:45.827102Z","shell.execute_reply.started":"2022-01-23T05:46:45.826723Z","shell.execute_reply":"2022-01-23T05:46:45.826756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_sub[-1]","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.828560Z","iopub.status.idle":"2022-01-23T05:46:45.829741Z","shell.execute_reply.started":"2022-01-23T05:46:45.829376Z","shell.execute_reply":"2022-01-23T05:46:45.829409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# G-Research\n\n* tutrial - [https://www.kaggle.com/cstein06/tutorial-to-the-g-research-crypto-competition](http://)\n\n# description\n\n* 15分後の価格の変動率を予測する\n* 常に市場の傾向が変動するので、確実な予測モデルを立てるのが難しい(傾向が非定常的)\n* オーバーフィッティングの可能性が高い\n* 通貨間での価格変動の関連性がある。特にビットコインはほかの通貨に影響を与えやすい。\n* 将来、価格がどのように動くかを予測することである。過去の価格の時系列データを学習データとして、価格が上がるか下がるか、またどの程度上がるか、すなわち資産リターンを予測\n\n### Data\n\n#### train.csv\n\n* timestamp: データのUNIX秒。すべて60秒間隔よって、一分ごとにデータが与えられている。\n* Asset_ID: asset_details.csvに書いている通貨IDと結びついており、通貨の種類の識別に使う。 (e.g. Asset_ID = 1 for Bitcoin)\n* Count: 前の一分間で取引された回数\n* Open: 始値 (in USD).\n* High: 前の一分間での最大の価格 (in USD).\n* Low: 前の一分間での最小の価格 (in USD).\n* Close: Close price 終値 (in USD).\n* Volume: 前の一分間の引通貨量(USD)\n* VWAP: 一定期間内での取引価格の、取引量による加重平均\n* Target: 15分前の価格との差をlogでとったもの Residual log-returns for the asset over a 15 minute horizon.\n\n#### asset_details.csv\n\n* Asset_ID 通貨ID\n* Weight 性能評価するときにその通貨の正答率がどのくらい加味されるかの重み\n* Asset_Name IDに結びついている通貨名\n\n\n### 評価方法\n\n\n\n\n\n# task\n\n1. 概要(overview)をしっかり読む\n2. 似ている過去のコンペを探し、参加し基本的な分析を行う\n3. 似たような大会のsolutionを読む\n4. 論文を読んでその分野の進捗を見逃さないようにする\n5. データを分析し安定したCVのモデルを構築する\n6. データ前処理、特徴量エンジニアリングを行い一定のモデルでCVを比較しいい特徴量エンジニアリングを探す\n7. モデルの予測と教師データを比較し分析、予測の難しいデータに対し考察\n8. 分析に基づき高性能なモデルをアンサンブルなどを取り入れて構築\n9.  データ解析、結果分析からより高度な予測の難しいサンプルを解決するモデルを設計\n10. 必要であれば前のステップに戻る\n\n# scores\n\n# else\n\n* supplemental_trainとtrainをくっつけて最後に学習\n* B (billion)\t1,000,000,000","metadata":{}},{"cell_type":"markdown","source":"# 可視化","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import numpy as np","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.831618Z","iopub.status.idle":"2022-01-23T05:46:45.832499Z","shell.execute_reply.started":"2022-01-23T05:46:45.832185Z","shell.execute_reply":"2022-01-23T05:46:45.832223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_folder = \"../input/g-research-crypto-forecasting/\"\n\n# crypto_df = reduce_mem_usage(pd.read_csv(data_folder + 'train.csv'))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.834353Z","iopub.status.idle":"2022-01-23T05:46:45.835280Z","shell.execute_reply.started":"2022-01-23T05:46:45.834959Z","shell.execute_reply":"2022-01-23T05:46:45.834991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_list = []\n# for Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n#     train = crypto_df[crypto_df[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n#     train = fe.conv_data(train, save_fet=False, save_name='feature')\n#     train_list.append(train)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.837132Z","iopub.status.idle":"2022-01-23T05:46:45.837996Z","shell.execute_reply.started":"2022-01-23T05:46:45.837649Z","shell.execute_reply":"2022-01-23T05:46:45.837680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fe = Feature()\n# for Asset_ID in range(13):#通貨別にデータを作りそれを通貨別でリストにDFを格納\n#     train = train_list[Asset_ID]\n    \n#     categorical_features = ['Embarked', 'Pclass', 'Sex']\n#     y_train = train['Target']\n#     X_train = train.drop('Target', axis=1)\n    \n#     #ct = Controller(Asset_ID)\n#     #ct.stacking_learn(categorical_features, X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.839759Z","iopub.status.idle":"2022-01-23T05:46:45.840657Z","shell.execute_reply.started":"2022-01-23T05:46:45.840315Z","shell.execute_reply":"2022-01-23T05:46:45.840347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import gresearch_crypto\n# #以下二つは1セッションで一度しか実行できない\n# env = gresearch_crypto.make_env()   # initialize the environment\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.842542Z","iopub.status.idle":"2022-01-23T05:46:45.843533Z","shell.execute_reply.started":"2022-01-23T05:46:45.843129Z","shell.execute_reply":"2022-01-23T05:46:45.843161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for (test_df, sample_prediction_df) in iter_test:\n    \n#     for Asset_ID in df_test.Asset_ID.unique():\n#         df_test = df_test[df_test[\"Asset_ID\"]==Asset_ID].set_index(\"timestamp\")\n        \n#         test = fe.conv_data(test, save_fet=False, save_name='feature')\n\n#         categorical_features = ['Embarked', 'Pclass', 'Sex']\n#         y_train = train['Target']\n        \n#         ct = Controller(Asset_ID)\n#         ct.stacking_predict(categorical_features, X_test)\n    \n#     #testと教師データを結合して特徴量を作る\n    \n#     sample_prediction_df['Target'] = 0  # make your predictions here\n#     env.predict(sample_prediction_df)   # register your predictions","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.845397Z","iopub.status.idle":"2022-01-23T05:46:45.846260Z","shell.execute_reply.started":"2022-01-23T05:46:45.845951Z","shell.execute_reply":"2022-01-23T05:46:45.845982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for (test_df, sample_prediction_df) in iter_test:\n#     sample_prediction_df['Target'] = 0\n#     env.predict(sample_prediction_df)\n#     print(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.848106Z","iopub.status.idle":"2022-01-23T05:46:45.849037Z","shell.execute_reply.started":"2022-01-23T05:46:45.848650Z","shell.execute_reply":"2022-01-23T05:46:45.848683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_folder = \"../input/g-research-crypto-forecasting/\"\n# crypto_df = pd.read_csv(data_folder + 'supplemental_train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.850693Z","iopub.status.idle":"2022-01-23T05:46:45.851755Z","shell.execute_reply.started":"2022-01-23T05:46:45.851411Z","shell.execute_reply":"2022-01-23T05:46:45.851445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas_profiling\n\n# train_list[0].profile_report()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.853486Z","iopub.status.idle":"2022-01-23T05:46:45.854376Z","shell.execute_reply.started":"2022-01-23T05:46:45.854047Z","shell.execute_reply":"2022-01-23T05:46:45.854079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# feature optimizer","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# data_folder = \"../input/g-research-crypto-forecasting/\"\n# crypto_df = pd.read_csv(data_folder + 'train.csv')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.856159Z","iopub.status.idle":"2022-01-23T05:46:45.857070Z","shell.execute_reply.started":"2022-01-23T05:46:45.856749Z","shell.execute_reply":"2022-01-23T05:46:45.856781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import optuna\n# import talib\n\n# price = np.array(crypto_df['Close'])\n# returns = np.array(crypto_df['Close'].shift(-15)) - price\n# #returns = np.array(crypto_df['Target'])\n\n# #ROCP {'timeperiod': 6} best param -0.0005372071418046449 best score\n# #MOM {'timeperiod': 127} best param -0.001971999282000135 best score\n# #RSI -0.0009095762807234701 3\n# #EMA {'timeperiod': 11} best param -0.00034526380519110374 best score\n\n# def objective(trial):\n#     timeperiod = trial.suggest_int('timeperiod', 2, 240)\n    \n#     df = talib.ROCP(price, timeperiod=timeperiod)\n#     returns_new = returns[~np.isnan(df)]\n#     df_new = df[~np.isnan(df)]\n#     returns_last = returns_new[~np.isnan(returns_new)]\n#     df_new = df_new[~np.isnan(returns_new)]\n#     ic = get_ic(df_new, returns_last)\n#     print(ic, timeperiod)\n#     return -abs(ic)\n\n# def get_ic(x, returns, normalize=True) -> float:\n#     \"\"\"\n#     :param np.ndarray x: 指標\n#     :param np.ndarray returns: リターン\n#     :param bool normalize: x をスケーリングするかどうか\n#     \"\"\"\n#     assert(len(x) == len(returns))\n#     x = (x - x.mean()) / x.std() if normalize else x\n#     returns = (returns - returns.mean()) / returns.std() if normalize else returns\n#     ic = np.corrcoef(x, returns)[0, 1]\n\n#     return ic\n\n# study = optuna.create_study(sampler=optuna.samplers.RandomSampler(seed=0))\n# study.optimize(objective, n_trials=100)\n# print(study.best_params,'best param')\n# print(study.best_value,'best score')","metadata":{"execution":{"iopub.status.busy":"2022-01-23T05:46:45.859483Z","iopub.status.idle":"2022-01-23T05:46:45.860417Z","shell.execute_reply.started":"2022-01-23T05:46:45.860082Z","shell.execute_reply":"2022-01-23T05:46:45.860113Z"},"trusted":true},"execution_count":null,"outputs":[]}]}